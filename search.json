[
  {
    "objectID": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html",
    "href": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html",
    "title": "GitHub Actions Introduction",
    "section": "",
    "text": "GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.\nGitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.\nGitHub provides Linux, Windows, and macOS virtual machines to run your workflows, or you can host your own self-hosted runners in your own data center or cloud infrastructure."
  },
  {
    "objectID": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#overview",
    "href": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#overview",
    "title": "GitHub Actions Introduction",
    "section": "",
    "text": "GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline. You can create workflows that build and test every pull request to your repository, or deploy merged pull requests to production.\nGitHub Actions goes beyond just DevOps and lets you run workflows when other events happen in your repository. For example, you can run a workflow to automatically add the appropriate labels whenever someone creates a new issue in your repository.\nGitHub provides Linux, Windows, and macOS virtual machines to run your workflows, or you can host your own self-hosted runners in your own data center or cloud infrastructure."
  },
  {
    "objectID": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#the-components-of-github-actions",
    "href": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#the-components-of-github-actions",
    "title": "GitHub Actions Introduction",
    "section": "The components of GitHub Actions",
    "text": "The components of GitHub Actions\nYou can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.\n\n\n\nThe_concept_of_GitHub_Actions\n\n\n\nWorkflows\nA workflow is a configurable automated process that will run one or more jobs.\nWorkflows are defined by a YAML file checked in to your repository and will run when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule.\nWorkflows are defined in the .github/workflows directory in a repository, and a repository can have multiple workflows, each of which can perform a different set of tasks.\n\n\n\n\n\n\nFor example, you can have one workflow to build and test pull requests, another workflow to deploy your application every time a release is created, and still another workflow that adds a label every time someone opens a new issue.\n\n\n\nYou can reference a workflow within another workflow. For more information, see “Reusing workflows.” For more information about workflows, see “Using workflows.”\n\n\n\nEvents\nAn event is a specific activity in a repository that triggers a workflow run.\n\n\n\n\n\n\nFor example, activity can originate from GitHub when someone creates a pull request, opens an issue, or pushes a commit to a repository. You can also trigger a workflow to run on a schedule, by posting to a REST API, or manually.\n\n\n\nFor a complete list of events that can be used to trigger workflows, see Events that trigger workflows.\n\n\n\nJobs\nA job is a set of steps in a workflow that is executed on the same runner.\nEach step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other.Since each step is executed on the same runner, you can share data from one step to another.\n\n\n\n\n\n\nFor example, you can have a step that builds your application followed by a step that tests the application that was built.\n\n\n\nYou can configure a job’s dependencies with other jobs; by default, jobs have no dependencies and run in parallel with each other. When a job takes a dependency on another job, it will wait for the dependent job to complete before it can run.\n\n\n\n\n\n\nFor example, you may have multiple build jobs for different architectures that have no dependencies, and a packaging job that is dependent on those jobs. The build jobs will run in parallel, and when they have all completed successfully, the packaging job will run.\n\n\n\nFor more information about jobs, see “Using jobs.”\n\n\n\nActions\n\n\n\n\n\n\nImportant\n\n\n\nAn action is a custom application for the GitHub Actions platform that performs a complex but frequently repeated task. Use an action to help reduce the amount of repetitive code that you write in your workflow files.\n\n\n\n\n\n\n\n\nAn action can pull your git repository from GitHub, set up the correct toolchain for your build environment, or set up the authentication to your cloud provider.\n\n\n\nYou can write your own actions, or you can find actions to use in your workflows in the GitHub Marketplace. For more information, see “Creating actions.”\n\n\n\nRunners\n\n\n\n\n\n\nImportant\n\n\n\nA runner is a server that runs your workflows when they’re triggered. Each runner can run a single job at a time.\n\n\n\n\n\n\n\n\nGitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows; each workflow run executes in a fresh, newly-provisioned virtual machine.\n\n\n\nGitHub also offers larger runners, which are available in larger configurations. For more information, see “About larger runners.” If you need a different operating system or require a specific hardware configuration, you can host your own runners. For more information about self-hosted runners, see “Hosting your own runners.”"
  },
  {
    "objectID": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#create-an-workflow",
    "href": "blogs/GitHub_Actions_Introduction/001_GitHub_Actions_Introduction.html#create-an-workflow",
    "title": "GitHub Actions Introduction",
    "section": "Create an workflow",
    "text": "Create an workflow\nGitHub Actions uses YAML syntax to define the workflow. Each workflow is stored as a separate YAML file in your code repository, in a directory named .github/workflows.\nYou can create an example workflow in your repository that automatically triggers a series of commands whenever code is pushed. 1. In your repository, create the .github/workflows/ directory to store your workflow files. 2. In the .github/workflows/ directory, create a new file called your_file_name.yml and write the code. 3. Commit these changes and push them to your GitHub repository.\nYour new GitHub Actions workflow file is now installed in your repository and will run automatically each time someone pushes a change to the repository. To see the details about a workflow’s execution history, see “Viewing the activity for a workflow run.”\n\nCode Block\n# This is a comment line, and it's for providing information about the workflow.\n# The name of the workflow is \"Hello world workflow.\"\nname: Hello word workflow\n\n# This section defines when the workflow should run.\non:\n  # This workflow runs when there's a code push to the \"main\" branch.\n  push:\n    branches:\n      - main\n  # This workflow also runs when there's a pull request to the \"main\" branch.\n  pull_request:\n    branches:\n      - main\n  # This line allows manual triggering of the workflow through the GitHub Actions web interface.\n  workflow_dispatch:\n\n# This section defines the jobs that this workflow contains.\njobs:\n  # This is the first job named \"hello.\"\n  hello:\n    # It runs on an \"ubuntu-latest\" runner, which is a virtual machine with Ubuntu.\n    runs-on: ubuntu-latest\n    steps:\n      # This step checks out (fetches) the code from the GitHub repository onto the runner.\n      - uses: actions/checkout@v3\n      # This step is named \"hello world.\"\n      - name: hello world\n        # It runs the command \"echo 'hello world'\" in the bash shell.\n        run: echo \"hello world\"\n        # This specifies that the shell to use is \"bash.\"\n\n  # This is the second job named \"goodbye.\"\n  goodbye:\n    # It also runs on an \"ubuntu-latest\" runner.\n    runs-on: ubuntu-latest\n    steps:\n      # This step is named \"good bye world.\"\n      - name: good bye world\n        # It runs the command \"echo 'goodbye world'\" in the bash shell.\n        run: echo \"goodbye world\"\n        # This specifies that the shell to use is \"bash.\"\n\n\n\nGitHub Interface\n     \nThe only difference between the above job ‘hello world’ and below job ‘good bye’ is here we are not using GitHub’s actions/checkout i.e. we’re not copying the copying the code from repository to runner. We’re running only simple shell command.\n\nExtra Utilities\n\n\n\n\nReference Material\n\n\nOfficial Documentation from GitHub for GitHub actions Personal GitHub Repository for trying the methods"
  },
  {
    "objectID": "blogs/SVM/SVM.html",
    "href": "blogs/SVM/SVM.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "SVM is a powerful supervised machine learning algorithm that works best on smaller dataset but on complex ones. Support Vector Machine, abbreviated as SVM can be used for both classification and regression tasks, but generally, they generally works best in classification problems."
  },
  {
    "objectID": "blogs/SVM/SVM.html#kernels-in-support-vector-machine",
    "href": "blogs/SVM/SVM.html#kernels-in-support-vector-machine",
    "title": "Support Vector Machines",
    "section": "Kernels in Support Vector Machine",
    "text": "Kernels in Support Vector Machine\nThe most interesting feature of SVM is that it can even work with a non-linear dataset and for this, we use “Kernel Trick” which makes it easier to classifies the points. Suppose we have a dataset like this:\n\nHere we see we cannot draw a single line or say hyperplane which can classify the points correctly. So what we do is try converting this lower dimension space to a higher dimension space using some quadratic functions which will allow us to find a decision boundary that clearly divides the data points. These functions which help us do this are called Kernels and which kernel to use is purely determined by hyperparameter tuning.\n\n\nDifferent Kernel Functions\nSome kernel functions which you can use in SVM are given below: #### 1. Polynomial Kernel Following is the formula for the polynomial kernel:\n\nHere d is the degree of the polynomial, which we need to specify manually.\nSuppose we have two features X1 and X2 and output variable as Y, so using polynomial kernel we can write it as:\n\nSo we basically need to find X12 , X22 and X1.X2, and now we can see that 2 dimensions got converted into 5 dimensions.\n\n\n2. Sigmoid Kernel\nWe can use it as the proxy for neural networks. Equation is:\n\nIt is just taking your input, mapping them to a value of 0 and 1 so that they can be separated by a simple straight line.\n\n\n\n3. RBF Kernel\nWhat it actually does is to create non-linear combinations of our features to lift your samples onto a higher-dimensional feature space where we can use a linear decision boundary to separate your classes It is the most used kernel in SVM classifications, the following formula explains it mathematically:\n\nwhere,\n\n\\(σ\\) is the variance and our hyperparameter\n\n\\(||X_₁ – X_₂||\\) is the Euclidean Distance between two points X₁ and X₂\n\n\n\n\n4. Bessel function kernel\nIt is mainly used for eliminating the cross term in mathematical functions. Following is the formula of the Bessel function kernel:\n\n\n\n5. Anova Kernel\nIt performs well on multidimensional regression problems. The formula for this kernel function is:\n\n\n\n\nHow to Choose the Right Kernel?\nI am well aware of the fact that you must be having this doubt about how to decide which kernel function will work efficiently for your dataset. It is necessary to choose a good kernel function because the performance of the model depends on it.\nChoosing a kernel totally depends on what kind of dataset are you working on. If it is linearly separable then you must opt. for linear kernel function since it is very easy to use and the complexity is much lower compared to other kernel functions. I’d recommend you start with a hypothesis that your data is linearly separable and choose a linear kernel function.\nYou can then work your way up towards the more complex kernel functions. Usually, we use SVM with RBF and linear kernel function because other kernels like polynomial kernel are rarely used due to poor efficiency. But what if linear and RBF both give approximately similar results? Which kernel do we choose now?\n\nExample\nLet’s understand this with the help of an example, for simplicity I’ll only take 2 features that mean 2 dimensions only. In the figure below I have plotted the decision boundary of a linear SVM on 2 features of the iris dataset:\n\nHere we see that a linear kernel works fine on this dataset, but now let’s see how will RBF kernel work.\n\nWe can observe that both the kernels give similar results, both work well with our dataset but which one should we choose? Linear SVM is a parametric model. A Parametric Model is a concept used to describe a model in which all its data is represented within its parameters.  In short, the only information needed to predict the future from the current value is the parameters.\nThe complexity of the RBF kernel grows as the training data size increases. In addition to the fact that it is more expensive to prepare RBF kernel, we also have to keep the kernel matrix around, and the projection into this “infinite” higher dimensional space where the data becomes linearly separable is more expensive as well during prediction. If the dataset is not linear then using linear kernel doesn’t make sense we’ll get a very low accuracy if we do so.\n\nSo for this kind of dataset, we can use RBF without even a second thought because it makes decision boundary like this:\n\n\n\nAdvantages of SVM\n\nSVM works better when the data is Linear\nIt is more effective in high dimensions\nWith the help of the kernel trick, we can solve any complex problem\nSVM is not sensitive to outliers\nCan help us with Image classification\n\n\n\nDisadvantages of SVM\n\nChoosing a good kernel is not easy\nIt doesn’t show good results on a big dataset\nThe SVM hyperparameters are Cost -C and gamma. It is not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact"
  },
  {
    "objectID": "blogs/SVM/SVM.html#conclusion",
    "href": "blogs/SVM/SVM.html#conclusion",
    "title": "Support Vector Machines",
    "section": "Conclusion",
    "text": "Conclusion\nIn this article, we looked at a very powerful machine learning algorithm, Support Vector Machine in detail. I discussed its concept of working, math intuition behind SVM, implementation in python, the tricks to classify non-linear datasets, Pros and cons, and finally, we solved a problem with the help of SVM."
  },
  {
    "objectID": "blogs/Unsupervised_Learning_Algorithms/Unsupervised Learning Algorithms.html",
    "href": "blogs/Unsupervised_Learning_Algorithms/Unsupervised Learning Algorithms.html",
    "title": "Unsupervised Learning Algorithms",
    "section": "",
    "text": "Imagine you give the computer a bunch of puzzle pieces without telling it what the picture should be. Unsupervised learning is like asking the computer to figure out if these pieces naturally form any meaningful patterns.\n\n\n\nunsupervised algorithms\n\n\nWhy Do We Use It?\nUnsupervised learning helps the computer discover hidden structures, group similar things, and uncover secrets within data.\nThe Heroes of Unsupervised Learning\n\nK-Means Clustering: Think of it as the computer’s way of putting similar items into groups. It’s like sorting fruits by their type without telling the computer what each fruit is.\nPrincipal Component Analysis (PCA): Imagine the computer finding the most important parts of a big puzzle. It’s used to reduce complex data into simpler pieces while keeping the essential information.\n\nHow It Works\n\nNo Teacher Needed: In unsupervised learning, there are no “right answers” provided during training. The computer explores data independently.\nDiscovering Patterns: It looks for similarities, differences, and structures within the data without us guiding it.\n\nIn the Real World\n\nSorting customer preferences in online shopping.\nGrouping news articles into topics without prior labels.\nIdentifying patterns in medical data for disease diagnosis.\n\nPros and Cons\n\nPros: It’s great for exploring unknown data structures and reducing data complexity.\nCons: You might need to interpret the results yourself, and it can be tricky if there are no clear patterns.\n\nTakeaway\nUnsupervised learning is like letting a detective loose in a mysterious room with no clues. It’s a valuable tool when you want the computer to find hidden gems and structures within your data, even when you don’t know exactly what to look for.\n\n\nReference Reading\n# Unsupervised Learning: Types, Applications & Advantages"
  },
  {
    "objectID": "blogs/GitHub_Actions_Continuous_Integration_CI/002_GitHub_Actions_Continuous_Integration_CI.html#before-continuous-integration-automation",
    "href": "blogs/GitHub_Actions_Continuous_Integration_CI/002_GitHub_Actions_Continuous_Integration_CI.html#before-continuous-integration-automation",
    "title": "GitHub Actions Continuous Integration CI",
    "section": "Before Continuous Integration Automation",
    "text": "Before Continuous Integration Automation\n\nSCM:\nBefore the introduction of modern tools like GitHub, source code management (SCM) primarily relied on older systems and practices. One of the earliest forms of SCM was manual version control, where developers would create backups or copies of their code and manually track changes. Here’s how SCM worked in the “olden days” before platforms like GitHub:\n\n\n\n\n\n\nSCM -&gt; Source Code Management System\n\n\n\n\nLocal Backups: Developers would often maintain local backups of their code on their own computers or servers. These backups were typically directories or folders containing different versions of the code.\nLabels or Tags: Developers would label or tag specific versions of their code with names or numbers to indicate milestones, releases, or significant changes.These labels helped in identifying and retrieving specific versions when needed.\nDocumentation: Detailed documentation about code changes, bug fixes, and new features was crucial. Developers would maintain written logs or files that described the modifications made in each version of the code.\nManual Collaboration: Collaboration among developers was challenging. They would often share code changes through emails, physical copies, or by copying files to shared network drives. Merging changes from different contributors was a manual and error-prone process.\nRisk of Overwrites: Without robust SCM tools, there was a significant risk of accidentally overwriting someone else’s changes, leading to code conflicts and errors.\nLimited Visibility: It was difficult to have a comprehensive view of the entire project’s history and changes. Developers had to rely on their local records and communication with colleagues to understand the evolution of the codebase.\nNo Central Repository: Unlike modern SCM systems that have a central repository (like GitHub), older SCM relied on each developer’s local copies and backups. This decentralized approach could lead to data loss if a developer’s machine failed or backups were not maintained properly.\n\n\n\n\nCentral Repository:\nBefore the introduction of centralized platforms like GitHub, central repositories for source code management (SCM) were typically set up and maintained using traditional version control systems or network file systems. Here’s how central repositories worked in the “olden days” before GitHub:\n\nVersion Control Systems (VCS): Organizations often used traditional VCS software like CVS (Concurrent Versions System) or SVN (Subversion) to create central repositories. These systems allowed developers to check in and check out code from a central location.\nLocal Copies: Developers had local copies of the code on their own machines, but the authoritative or master copy resided in the central repository. Developers would make changes to their local copies and then commit those changes to the central repository.\nAccess Control: Access to the central repository was controlled through user authentication and permissions. Typically, only authorized individuals or teams had write access to the central repository, while others had read-only access.\nCheck-In and Check-Out: Developers would check out code from the central repository to work on it. When they were done making changes, they would check the code back into the central repository. This process ensured that changes were tracked and managed centrally.\nVersion History: The central repository maintained a version history of the code. Each check-in or commit created a new version, allowing developers to see how the code evolved over time.\nBranching and Merging: These systems supported branching and merging, but it was often a more manual and less user-friendly process compared to modern distributed version control systems like Git. Merging code changes from different branches could be complex.\nCommunication: Collaboration and communication among developers were essential. Developers needed to coordinate with each other to avoid conflicts when making changes to the central repository.\nBackup and Recovery: Maintaining backups of the central repository was crucial because the loss of the central repository could result in significant data loss. Regular backups were often performed to ensure data integrity.\nLimited Collaboration Features: Unlike modern platforms like GitHub, these central repositories typically lacked collaboration features such as issue tracking, pull requests, and code review tools. Collaboration mainly happened outside the central repository using email or other communication tools.\n\n\n\n\nDeveloper Team\nFor the understanding purpose, we are working on a application with the olden system as discussed above with [[002_GitHub_Actions_Continuous_Integration_CI#SCM| SCM]] and [[002_GitHub_Actions_Continuous_Integration_CI#Central Repository| repository system]].\nOur developers are working on features, every developer is working on individual feature.\nFor the time sake, we say every developer is responsible for the feature he is assign to, he doesn’t know about the working code of other developer\nAfter completing the feature, the developer will check in the code into the central repository. Similarly other developers will also do the same. Once all the developers have check in the code, there work is complete for the time being.\n\n\n\nIntegration Team\nHere we will have integration team for the integration of code, they will checkout the code for each feature from the central repository and will build the application on a machine.\nThe goal of integration team is: - Integrate all the code for the feature and build it. - Run the code. - Test the integration quality.\nWhat doesn’t come under their scope: - Test the features according to the specification and design.\nOnce they finished the integration, they will share the details with the QA (Quality Assurance) team for further testing.\n\n\n\nQuality Assurance (QA) Team\nAfter the integration of the code, and building. The QA team will test the features if they meet the specification and business requirements.\nIf they find any problem with the features, they will send the code back to the developer and the process will repeat again.\n(developer team) feature development/bug fixing –&gt; code integration –&gt; quality assurance tests.\n\n\n\nProblem with Manual Approach\n\nTime consuming\nExtra manpower\nCost expensive."
  },
  {
    "objectID": "blogs/GitHub_Actions_Continuous_Integration_CI/002_GitHub_Actions_Continuous_Integration_CI.html#with-continuous-integration",
    "href": "blogs/GitHub_Actions_Continuous_Integration_CI/002_GitHub_Actions_Continuous_Integration_CI.html#with-continuous-integration",
    "title": "GitHub Actions Continuous Integration CI",
    "section": "With Continuous Integration",
    "text": "With Continuous Integration\n\nIn this approach we will use the same problem statement of building an application.\nHere we will use git as SCM and GitHub as Central Repository\nHere the developers will start working on the features, and with each code update they will check in the code in the central repository. In this approach, the developer will not have to worry about the other developers, he can work in his own feature branch at his own pace. He can also track the progress of other developers."
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "",
    "text": "Install colab-xterm package: python     !pip install colab-xterm\nLoad the xterm extension: python     %load_ext colabxterm\nOpen a terminal window: python     %xterm\n\n\n\n\n\nIn the terminal, run the following to install Ollama: bash     curl -fsSL https://ollama.com/install.sh | sh\nThen run the following command in terminal to pull the model from ollma model repository and host it on google colab: bash     ollama serve & ollma pull llama3.2"
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#accessing-ollama-in-free-google-colab-session",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#accessing-ollama-in-free-google-colab-session",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "",
    "text": "Install colab-xterm package: python     !pip install colab-xterm\nLoad the xterm extension: python     %load_ext colabxterm\nOpen a terminal window: python     %xterm\n\n\n\n\n\nIn the terminal, run the following to install Ollama: bash     curl -fsSL https://ollama.com/install.sh | sh\nThen run the following command in terminal to pull the model from ollma model repository and host it on google colab: bash     ollama serve & ollma pull llama3.2"
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#creating-payload-and-inference-via-local-url",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#creating-payload-and-inference-via-local-url",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "2. Creating Payload and Inference via Local URL",
    "text": "2. Creating Payload and Inference via Local URL\n\nSample Payload Format\nMODEL = \"llama3.2\"\nOLLAMA_API = \"http://localhost:11434/api/chat\"\nHEADERS = {\"Content-Type\": \"application/json\"}\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n]\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": messages,\n    \"stream\": False\n}\n\n\nSending Payload via requests\nimport requests\nresponse = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\nprint(response.json()[\"message\"][\"content\"])"
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#using-the-python-ollama-library",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#using-the-python-ollama-library",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "3. Using the Python ollama Library",
    "text": "3. Using the Python ollama Library\n\nInstallation (if needed)\n!pip install ollama\n\n\nInference Example\nimport ollama\nresponse = ollama.chat(model=MODEL, messages=messages)\nprint(response[\"message\"][\"content\"])"
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#using-the-openai-library-for-inference-with-ollama",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#using-the-openai-library-for-inference-with-ollama",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "4. Using the openai Library for Inference with Ollama",
    "text": "4. Using the openai Library for Inference with Ollama\n\nInstallation\n!pip install openai\n\n\nSet API Key and Base URL (Ollama emulates OpenAI API)\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\nresponse = client.chat.completions.create(\n    model=\"llama3.2\",\n    messages=[{\"role\": \"user\", \"content\": \"What is Artificial Intelligence?\"}]\n)\n\nprint(response.choices[0].message.content)"
  },
  {
    "objectID": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#pulling-and-using-deepseek-model",
    "href": "blogs/Day 2 Inferencing Local Ollama/Day 2 Inferencing Local Ollama.html#pulling-and-using-deepseek-model",
    "title": "Accessing Ollama in Free Google Colab Session",
    "section": "5. Pulling and Using DeepSeek Model",
    "text": "5. Pulling and Using DeepSeek Model\n\nPull DeepSeek Model\n!ollama pull deepseek-coder:6.7b-instruct\n\n\nInference with DeepSeek Model\nresponse = requests.post(\"http://localhost:11434/api/generate\", json={\n    \"model\": \"deepseek-coder:6.7b-instruct\",\n    \"prompt\": \"Write a Python function to compute factorial.\"\n})\nprint(response.json()[\"response\"])\nOr using Python ollama package:\nresponse = ollama.chat(model=\"deepseek-coder:6.7b-instruct\", messages=[\n    {\"role\": \"user\", \"content\": \"Explain decorators in Python\"}\n])\nprint(response[\"message\"][\"content\"])"
  },
  {
    "objectID": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html",
    "href": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html",
    "title": "Introduction to Algorithm",
    "section": "",
    "text": "In the context of machine learning and deep learning, an algorithm is like a set of step-by-step instructions that a computer follows to learn from data or make predictions.\nIt’s a recipe for the computer to process information, find patterns, and make decisions.\nThink of it as a smart way for a computer to solve problems or make sense of complex data.\nThese algorithms help us build models that can recognize patterns, understand language, or even play games, among many other things.\n\n\n\n\nIn machine learning and deep learning, there are several types of algorithms, each with its own purpose and characteristics. Here’s a simple overview of some common types:\n\n[[Supervised Learning Algorithms]]:\n\nThese algorithms learn from labeled data, where the input and the desired output are known.\nCommon algorithms include Linear Regression, Decision Trees, and Support Vector Machines (SVM). \n\n[[Unsupervised Learning Algorithms]]:\n\nThese algorithms work with unlabeled data, aiming to find hidden patterns or group similar data points.\nExamples include K-Means Clustering and Principal Component Analysis (PCA).\n\nDeep Learning Algorithms:\n\nDeep learning is a subset of machine learning that uses neural networks with many layers (deep networks) to learn from data.\nCommon deep learning algorithms include Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequence data.\n\nReinforcement Learning Algorithms:\n\nThese algorithms are used in scenarios where an agent learns to take actions to maximize rewards in an environment.\nWell-known algorithms include Q-Learning and Deep Q-Networks (DQNs).\n\nSemi-Supervised and Self-Supervised Learning:\n\nThese approaches combine elements of supervised and unsupervised learning, making use of both labeled and unlabeled data.\n\nTransfer Learning:\n\nTransfer learning involves using a pre-trained model on a related task as a starting point for a new task, saving training time and resources.\n\nEnsemble Methods:\n\nEnsemble methods combine multiple models to improve prediction accuracy. Examples include Random Forests and Gradient Boosting.\n\nNatural Language Processing (NLP) Algorithms:\n\nThese are specialized algorithms for working with text data, such as sentiment analysis and named entity recognition.\n\nDeep Reinforcement Learning:\n\nThis combines deep learning and reinforcement learning, often used in complex tasks like game playing (e.g., AlphaGo).\n\nDimensionality Reduction:\n\nAlgorithms like t-SNE and UMAP are used to reduce the number of features in high-dimensional data while preserving important relationships.\n\n\nThese are just some of the many types of algorithms used in machine learning and deep learning. The choice of algorithm depends on the specific problem you’re trying to solve and the type of data you have."
  },
  {
    "objectID": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html#introduction-to-algorithm",
    "href": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html#introduction-to-algorithm",
    "title": "Introduction to Algorithm",
    "section": "",
    "text": "In the context of machine learning and deep learning, an algorithm is like a set of step-by-step instructions that a computer follows to learn from data or make predictions.\nIt’s a recipe for the computer to process information, find patterns, and make decisions.\nThink of it as a smart way for a computer to solve problems or make sense of complex data.\nThese algorithms help us build models that can recognize patterns, understand language, or even play games, among many other things.\n\n\n\n\nIn machine learning and deep learning, there are several types of algorithms, each with its own purpose and characteristics. Here’s a simple overview of some common types:\n\n[[Supervised Learning Algorithms]]:\n\nThese algorithms learn from labeled data, where the input and the desired output are known.\nCommon algorithms include Linear Regression, Decision Trees, and Support Vector Machines (SVM). \n\n[[Unsupervised Learning Algorithms]]:\n\nThese algorithms work with unlabeled data, aiming to find hidden patterns or group similar data points.\nExamples include K-Means Clustering and Principal Component Analysis (PCA).\n\nDeep Learning Algorithms:\n\nDeep learning is a subset of machine learning that uses neural networks with many layers (deep networks) to learn from data.\nCommon deep learning algorithms include Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequence data.\n\nReinforcement Learning Algorithms:\n\nThese algorithms are used in scenarios where an agent learns to take actions to maximize rewards in an environment.\nWell-known algorithms include Q-Learning and Deep Q-Networks (DQNs).\n\nSemi-Supervised and Self-Supervised Learning:\n\nThese approaches combine elements of supervised and unsupervised learning, making use of both labeled and unlabeled data.\n\nTransfer Learning:\n\nTransfer learning involves using a pre-trained model on a related task as a starting point for a new task, saving training time and resources.\n\nEnsemble Methods:\n\nEnsemble methods combine multiple models to improve prediction accuracy. Examples include Random Forests and Gradient Boosting.\n\nNatural Language Processing (NLP) Algorithms:\n\nThese are specialized algorithms for working with text data, such as sentiment analysis and named entity recognition.\n\nDeep Reinforcement Learning:\n\nThis combines deep learning and reinforcement learning, often used in complex tasks like game playing (e.g., AlphaGo).\n\nDimensionality Reduction:\n\nAlgorithms like t-SNE and UMAP are used to reduce the number of features in high-dimensional data while preserving important relationships.\n\n\nThese are just some of the many types of algorithms used in machine learning and deep learning. The choice of algorithm depends on the specific problem you’re trying to solve and the type of data you have."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Brand Detection\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nVegetable Recognition\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nSadashiv\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html",
    "title": "Vegetable Recognition",
    "section": "",
    "text": "This notebook demonstrates a deep learning approach to classifying vegetable types from images, a task with applications in agriculture and retail. The model is built using PyTorch and features a custom ResNet9 architecture tailored for effective image classification. The notebook outlines the key steps in data preprocessing, model architecture, training, and evaluation, providing a comprehensive walkthrough of the process."
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#downloading-the-dataset",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#downloading-the-dataset",
    "title": "Vegetable Recognition",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\n\n\nCode\n# installing opendatasets to download the dataset from kaggle\n%pip install opendatasets -q\n\n\n\n\nCode\n# installing albumentations library for image transformation\n%pip install albumentations -q\n\n\n\n\nCode\n# importing opendatasets\nimport opendatasets as od\n\n# dataset url path\nurl = \"https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset?datasetId=1817999&sortBy=voteCount\"\n\n# download the dataset\nod.download(url)\n\n\nDownloading vegetable-image-dataset.zip to ./vegetable-image-dataset\n\n\n100%|██████████| 534M/534M [00:04&lt;00:00, 117MB/s]\n\n\n\n\n\n\n\nCode\n# imorting dependancies\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport albumentations as A\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport torch.nn as nn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nimport os\n\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#introduction-to-dataset",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#introduction-to-dataset",
    "title": "Vegetable Recognition",
    "section": "Introduction to Dataset",
    "text": "Introduction to Dataset\n\n\nCode\n# dataset path\nDATASET_PATH = \"./vegetable-image-dataset/Vegetable Images\"\nTRAIN_DATASET = DATASET_PATH + '/train'\nVALIDATION_DATASET = DATASET_PATH + '/validation'\n\nprint(f\"Total Categories in the dataset: {len(os.listdir(TRAIN_DATASET))}\")\n\nCATEGORIES_COUNT = {}\nCATEGORIES_LIST = os.listdir(TRAIN_DATASET)\n\nfor i in CATEGORIES_LIST:\n  CATEGORIES_COUNT[i] = len(os.listdir(str(TRAIN_DATASET) + \"/\" +str(i)))\n\nprint(CATEGORIES_COUNT)\nprint('\\n')\n\nCATEGORIES_COUNT = list(CATEGORIES_COUNT.values())\nplt.pie(CATEGORIES_COUNT, labels=CATEGORIES_LIST, autopct='%1.1f%%');\n\n\nTotal Categories in the dataset: 15\n{'Cucumber': 1000, 'Capsicum': 1000, 'Papaya': 1000, 'Tomato': 1000, 'Cabbage': 1000, 'Pumpkin': 1000, 'Bitter_Gourd': 1000, 'Radish': 1000, 'Broccoli': 1000, 'Cauliflower': 1000, 'Bean': 1000, 'Carrot': 1000, 'Bottle_Gourd': 1000, 'Potato': 1000, 'Brinjal': 1000}"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#importing-the-dataset-into-pytorch",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#importing-the-dataset-into-pytorch",
    "title": "Vegetable Recognition",
    "section": "Importing the dataset into pytorch",
    "text": "Importing the dataset into pytorch\n\n\nCode\n# importing pytorch\nimport torch\n\n# checking the version of the torch\nprint(f\"torch version: {torch.__version__}\")\n\n# Importing Imagefolder\nfrom torchvision.datasets import ImageFolder\n\n\ntorch version: 2.0.1+cu118\n\n\n\n\nCode\ntrain_dataset = ImageFolder(root=TRAIN_DATASET)\n\n# total images in the train_dataset\nprint(f\"Total images in train dataset: {len(train_dataset)}\\n\")\n\n# checking sample image from the train dataset\nimg, label = train_dataset[10]\n\nprint(f\"Sample image from train dataset\")\nplt.imshow(img)\nplt.axis('OFF');\n\n\nTotal images in train dataset: 15000\n\nSample image from train dataset\n\n\n\n\n\n\n\n\n\n\n\nCode\nvalidation_dataset = ImageFolder(root=VALIDATION_DATASET)\n\n# total images in the validation_dataset\nprint(f\"Total images in train dataset: {len(validation_dataset)}\\n\")\n\n# checking sample image from the train dataset\nimg, label = validation_dataset[2000]\n\nprint(f\"Sample image from validation_dataset\")\nplt.imshow(img)\nplt.axis('OFF');\n\n\nTotal images in train dataset: 3000\n\nSample image from validation_dataset\n\n\n\n\n\n\n\n\n\nImage transformation\n\n\nCode\n# Creating a class for image transformation with albumentations library\nclass ImageFolder(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(ImageFolder, self).__init__()\n        self.data = []\n        self.root_dir = root_dir\n        self.transform = transform\n        self.class_names = os.listdir(root_dir)\n\n        for index, name in enumerate(self.class_names):\n            files = os.listdir(os.path.join(root_dir, name))\n            self.data += list(zip(files, [index] * len(files)))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        img_file, label = self.data[index]\n        root_and_dir = os.path.join(self.root_dir, self.class_names[label])\n        image = np.array(Image.open(os.path.join(root_and_dir, img_file)))\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image)\n            image = augmentations[\"image\"]\n\n        return image, label\n\n\ntransform = A.Compose(\n    [\n        A.Resize(width=64, height=64),\n        A.Rotate(limit=40, p=0.9, border_mode=cv2.BORDER_CONSTANT),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\n        A.OneOf(\n            [\n                A.Blur(blur_limit=3, p=0.5),\n                A.ColorJitter(p=0.5),\n            ],\n            p=1.0,\n        ),\n        A.Normalize(\n            mean=[0, 0, 0],\n            std=[1, 1, 1],\n            max_pixel_value=255,\n        ),\n        ToTensorV2(),\n    ]\n)\n\n\n\n\nCode\n# Applying transformation on training_dataset and validation_dataset\ntrain_dataset = ImageFolder(root_dir=TRAIN_DATASET, transform=transform)\nvalidation_dataset = ImageFolder(root_dir=VALIDATION_DATASET, transform=transform)\n\n\n\n\nCode\n# checking a random image from train_dataset\nimg, label = train_dataset[100]\n\nprint(f\"Shape of the image: {img.shape}\") # checking the size of the image\n\nprint(\"\\nThe image after transformation\")\nprint(train_dataset[100]) # the pixel values are noramalized\nprint(\"\\n\")\n\nprint(f\"Sample image\")\nplt.imshow(img.permute((1,2,0))); #This module returns a view of the tensor input with its dimensions permuted.\n\n\nShape of the image: torch.Size([3, 64, 64])\n\nThe image after transformation\n(tensor([[[0.7098, 0.6980, 0.7216,  ..., 0.6157, 0.6000, 0.6118],\n         [0.7137, 0.7020, 0.7255,  ..., 0.6157, 0.5922, 0.5961],\n         [0.7216, 0.7098, 0.7255,  ..., 0.6078, 0.5686, 0.5686],\n         ...,\n         [0.7765, 0.7804, 0.8000,  ..., 0.7490, 0.7922, 0.7882],\n         [0.7686, 0.7882, 0.8235,  ..., 0.7843, 0.7882, 0.7843],\n         [0.7569, 0.7843, 0.8275,  ..., 0.8078, 0.7843, 0.7843]],\n\n        [[0.6431, 0.6392, 0.6706,  ..., 0.4431, 0.4353, 0.4353],\n         [0.6510, 0.6431, 0.6745,  ..., 0.4392, 0.4314, 0.4314],\n         [0.6627, 0.6510, 0.6706,  ..., 0.4314, 0.4196, 0.4196],\n         ...,\n         [0.6078, 0.6039, 0.6039,  ..., 0.6627, 0.7098, 0.7059],\n         [0.5804, 0.5804, 0.5725,  ..., 0.6980, 0.7059, 0.6980],\n         [0.5725, 0.5686, 0.5529,  ..., 0.7255, 0.6980, 0.7020]],\n\n        [[0.8235, 0.8118, 0.8431,  ..., 0.4627, 0.4667, 0.4627],\n         [0.8275, 0.8196, 0.8471,  ..., 0.4588, 0.4627, 0.4588],\n         [0.8392, 0.8275, 0.8431,  ..., 0.4471, 0.4549, 0.4549],\n         ...,\n         [0.6353, 0.6314, 0.6275,  ..., 0.7647, 0.8039, 0.7961],\n         [0.6078, 0.6078, 0.5922,  ..., 0.8000, 0.7961, 0.7804],\n         [0.5961, 0.5922, 0.5725,  ..., 0.8235, 0.7882, 0.7765]]]), 0)\n\n\nSample image"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#dataloader",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#dataloader",
    "title": "Vegetable Recognition",
    "section": "DataLoader",
    "text": "DataLoader\n\n\nCode\n# importing DataLoader\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\nprint(f\"cpu_count in machine: {NUM_WORKERS}\")\n\n# dataloader for training dataset\ntrain_dataloader = DataLoader(dataset = train_dataset,\n                              batch_size=BATCH_SIZE,\n                              num_workers=NUM_WORKERS,\n                              shuffle=True)\n\n# dataloader for validation dataset\nvalidation_dataloader = DataLoader(dataset=validation_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   num_workers=NUM_WORKERS,\n                                   shuffle=False)\n\n\ncpu_count in machine: 2\n\n\n\n\nCode\n# Get image and label from train_dataloader\ntrain_dataloder_img, train_dataloder_label = next(iter(train_dataloader))\n\n# Print out the shapes\ntrain_dataloder_img.shape, train_dataloder_label.shape\n\n\n(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#visualize-some-images-from-dataloader",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#visualize-some-images-from-dataloader",
    "title": "Vegetable Recognition",
    "section": "Visualize some images from dataloader",
    "text": "Visualize some images from dataloader\n\n\nCode\nfrom torchvision.utils import make_grid # Make a grid of images.\n\ndef show_batch(data_loader):\n  for images, labels in data_loader:\n    fig, ax = plt.subplots(figsize = (15,8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n    break\n\nshow_batch(train_dataloader)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# device setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n\n'cuda'"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#model-training",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#model-training",
    "title": "Vegetable Recognition",
    "section": "Model Training",
    "text": "Model Training\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Create class for model training and evaluation\nclass ImageClassificationBase(nn.Module): ## --&gt; nn.Module--&gt; Base class for all neural network modules. --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n  def training_step(self, batch):\n    images, labels = batch\n    images, labels = images.to(device), labels.to(device)\n    out = self(images) # Generate predictions\n    loss = F.cross_entropy(input= out, # Predicted unnormalized logits\n                           target = labels) # Ground truth class indices or class probabilities\n    return loss\n\n  def validation_step(self, batch):\n    images, labels = batch\n    images, labels = images.to(device), labels.to(device)\n    out = self(images) # Generate predictions\n    loss = F.cross_entropy(input = out, # Predicted unnormalized logits\n                           target = labels) # Ground truth class indices or class probabilities\n    acc = accuracy(outputs = out, # Calculate the accuracy\n                   labels = labels)\n    return {'validation_loss': loss.detach(), 'validation_accuracy':acc}\n\n  def validation_epoch_end(self, outputs):\n    batch_losses = [x['validation_loss'] for x in outputs]\n    # combine the losses\n    epoch_loss = torch.stack(batch_losses).mean() # PyTorch torch.stack() method joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n    batch_accuracy = [x['validation_accuracy'] for x in outputs]\n    epoch_accuracy = torch.stack(batch_accuracy).mean()\n    return {'validation_loss':epoch_loss.item(), 'validation_accuracy':epoch_accuracy.item()}\n\n  # printing the results\n  def epoch_end(self, epoch, result):\n    print(f\"Epoch {epoch},\\n train_loss:{result['train_loss']}, \\n validation_loss: {result['validation_loss']}, \\n validation_accuracy: {result['validation_accuracy']}\")\n\ndef accuracy(outputs, labels):\n  _, preds = torch.max(outputs, dim=1)\n  return torch.tensor(torch.sum(preds == labels).item()/ len(preds))\n\n\n\n\nCode\n@torch.no_grad() # Context-manager that disabled gradient calculation.\n\ndef evaluate(model, validation_dataloader):\n  \"\"\" Evaluate the model's performance on the validation dataset\"\"\"\n  model.eval()\n  outputs = [model.validation_step(batch) for batch in validation_dataloader]\n  return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n  history = []\n  optimizer = opt_func(model.parameters(), lr)\n  for epoch in range(epochs):\n    # training\n    model.train()\n    train_loss = []\n    for batch in train_loader:\n      loss = model.training_step(batch) # training the model for each batch\n      train_loss.append(loss) # collecting the loss\n      loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.--&gt; https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch-tensor-backward\n      optimizer.step() # Performs a single optimization step (parameter update). --&gt; https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html\n      optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero. --&gt; https://stackoverflow.com/a/48009142\n                            # official_doc --&gt; https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n\n    # validation\n    result = evaluate(model, validation_dataloader)\n    result['train_loss'] = torch.stack(train_loss).mean().item()\n    history.append(result)\n  return history\n\n\n\n\nCode\n##Model Building\ndef conv_block(in_channels, out_channels, pool=False):\n\n  layers = [nn.Conv2d(in_channels=in_channels,    # Number of channels in the input image\n                      out_channels=out_channels,  # Number of channels produced by the convolution\n                      kernel_size=3, # Size of the convolving kernel\n                      padding=1), # Padding added to all four sides of the input\n            nn.BatchNorm2d(num_features=out_channels), # num_features (int)--&gt;'C'from an expected input of size (N,C,H,W) --&gt; https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n            nn.ReLU(inplace =True)] #-&gt; https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu\n  if pool: layers.append(nn.MaxPool2d(kernel_size=2)) # --&gt; https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n  return nn.Sequential(*layers)\n\n\n\n\nCode\nclass ResNet9(ImageClassificationBase):\n  def __init__(self, in_channels, num_classes):\n    super().__init__()\n    # input: 32 x 3 x 64 x 64\n    self.conv1 = conv_block(in_channels, 64) # 32 x 64 x 64 x 64\n    self.conv2 = conv_block(64, 128, pool=True) # 32 x 128 x 32 x 32\n    self.res1 = nn.Sequential(conv_block(128, 128), # 32 x 128 x 32 x 32\n                              conv_block(128, 128)) # 32 x 128 x 32 x 32\n\n    self.conv3 = conv_block(128, 256, pool=True) # 32 X 256 x 16 x 16\n    self.conv4 = conv_block(256, 512, pool=True) # 32 x 256 x 8 x 8\n    self.res2 = nn.Sequential(conv_block(512, 512), # 32 x 512 x 8 x 8 --&gt; residual_blocks --&gt; ## https://towardsdatascience.com/resnets-residual-blocks-deep-residual-learning-a231a0ee73d2#:~:text=A%20residual%20block%20is%20a,layer%20in%20the%20main%20path.\n                              conv_block(512, 512)) # 32 x 512 x 8 x 8\n\n    self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d(1), # 32 x 512 x 1 x 1 --&gt; official_documenataion --&gt;https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d --&gt; simplified_version--&gt; https://stackoverflow.com/a/55869581\n                                    nn.Flatten(), # Flattens a contiguous range of dims into a tensor --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten --&gt;simplified_version --&gt; https://www.tutorialspoint.com/how-to-flatten-an-input-tensor-by-reshaping-it-in-pytorch\n                                    nn.Dropout(0.2), # During training, randomly zeroes some of the elements of the input tensor --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout --&gt; https://www.geeksforgeeks.org/dropout-in-neural-networks/\n                                    nn.Linear(in_features = 512,\n                                              out_features = num_classes)) # Applies a linear transformation to the incoming data --&gt;https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear --&gt;https://stackoverflow.com/a/54924812\n\n  def forward(self, xb):\n    out = self.conv1(xb)\n    out = self.conv2(out)\n    out = self.res1(out) + out\n    out = self.conv3(out)\n    out = self.conv4(out)\n    out = self.res2(out) + out\n    out = self.classifier(out)\n    return out\n\n\n\n\nCode\nmodel = ResNet9(3, len(train_dataset.class_names)).to(device)\nmodel\n\n\nResNet9(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res1): Sequential(\n    (0): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (conv3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res2): Sequential(\n    (0): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=1)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=512, out_features=15, bias=True)\n  )\n)\n\n\n\n\nCode\n# Install torchinfo, import if it's available\n# Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch,\n# similar to Tensorflow's model.summary() API to view the visualization of the model, which is helpful while debugging your network.\n\ntry:\n  import torchinfo\nexcept:\n  !pip install torchinfo\n  import torchinfo\n\nfrom torchinfo import summary\nsummary(model, input_size=[1, 3, 64, 64])\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting torchinfo\n  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.8.0\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet9                                  [1, 15]                   --\n├─Sequential: 1-1                        [1, 64, 64, 64]           --\n│    └─Conv2d: 2-1                       [1, 64, 64, 64]           1,792\n│    └─BatchNorm2d: 2-2                  [1, 64, 64, 64]           128\n│    └─ReLU: 2-3                         [1, 64, 64, 64]           --\n├─Sequential: 1-2                        [1, 128, 32, 32]          --\n│    └─Conv2d: 2-4                       [1, 128, 64, 64]          73,856\n│    └─BatchNorm2d: 2-5                  [1, 128, 64, 64]          256\n│    └─ReLU: 2-6                         [1, 128, 64, 64]          --\n│    └─MaxPool2d: 2-7                    [1, 128, 32, 32]          --\n├─Sequential: 1-3                        [1, 128, 32, 32]          --\n│    └─Sequential: 2-8                   [1, 128, 32, 32]          --\n│    │    └─Conv2d: 3-1                  [1, 128, 32, 32]          147,584\n│    │    └─BatchNorm2d: 3-2             [1, 128, 32, 32]          256\n│    │    └─ReLU: 3-3                    [1, 128, 32, 32]          --\n│    └─Sequential: 2-9                   [1, 128, 32, 32]          --\n│    │    └─Conv2d: 3-4                  [1, 128, 32, 32]          147,584\n│    │    └─BatchNorm2d: 3-5             [1, 128, 32, 32]          256\n│    │    └─ReLU: 3-6                    [1, 128, 32, 32]          --\n├─Sequential: 1-4                        [1, 256, 16, 16]          --\n│    └─Conv2d: 2-10                      [1, 256, 32, 32]          295,168\n│    └─BatchNorm2d: 2-11                 [1, 256, 32, 32]          512\n│    └─ReLU: 2-12                        [1, 256, 32, 32]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 16, 16]          --\n├─Sequential: 1-5                        [1, 512, 8, 8]            --\n│    └─Conv2d: 2-14                      [1, 512, 16, 16]          1,180,160\n│    └─BatchNorm2d: 2-15                 [1, 512, 16, 16]          1,024\n│    └─ReLU: 2-16                        [1, 512, 16, 16]          --\n│    └─MaxPool2d: 2-17                   [1, 512, 8, 8]            --\n├─Sequential: 1-6                        [1, 512, 8, 8]            --\n│    └─Sequential: 2-18                  [1, 512, 8, 8]            --\n│    │    └─Conv2d: 3-7                  [1, 512, 8, 8]            2,359,808\n│    │    └─BatchNorm2d: 3-8             [1, 512, 8, 8]            1,024\n│    │    └─ReLU: 3-9                    [1, 512, 8, 8]            --\n│    └─Sequential: 2-19                  [1, 512, 8, 8]            --\n│    │    └─Conv2d: 3-10                 [1, 512, 8, 8]            2,359,808\n│    │    └─BatchNorm2d: 3-11            [1, 512, 8, 8]            1,024\n│    │    └─ReLU: 3-12                   [1, 512, 8, 8]            --\n├─Sequential: 1-7                        [1, 15]                   --\n│    └─AdaptiveAvgPool2d: 2-20           [1, 512, 1, 1]            --\n│    └─Flatten: 2-21                     [1, 512]                  --\n│    └─Dropout: 2-22                     [1, 512]                  --\n│    └─Linear: 2-23                      [1, 15]                   7,695\n==========================================================================================\nTotal params: 6,577,935\nTrainable params: 6,577,935\nNon-trainable params: 0\nTotal mult-adds (G): 1.52\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 24.12\nParams size (MB): 26.31\nEstimated Total Size (MB): 50.48\n==========================================================================================\n\n\n\n\nCode\ntorch.cuda.empty_cache() # Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n# official documentation --&gt; https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache\n\nfor batch in train_dataloader:\n  images, labels = batch\n  print(f\"image shape: \", images.shape)\n  print(f\"images device: \", images.device)\n  preds = model(images.to(device))\n  print('preds.shape', preds.shape)\n  break\n\n\nimage shape:  torch.Size([32, 3, 64, 64])\nimages device:  cpu\npreds.shape torch.Size([32, 15])\n\n\n\n\nCode\nhistory = [evaluate(model.to(device),\n                    validation_dataloader)]\nhistory\n\n\n[{'validation_loss': 2.708531379699707,\n  'validation_accuracy': 0.06648936122655869}]\n\n\n\n\nCode\nhistory += fit(epochs=5,\n               lr=0.01,\n               model=model,\n               train_loader=train_dataloader,\n               val_loader=validation_dataloader,\n               opt_func=torch.optim.Adam)\n\n\n\n\nCode\ndef plot_accuracies(history):\n    accuracies = [x['validation_accuracy'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\nplot_accuracies(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhistory += fit(epochs=10,\n               lr=0.01,\n               model=model,\n               train_loader=train_dataloader,\n               val_loader=validation_dataloader,\n               opt_func=torch.optim.Adam)\n\n\n\n\nCode\ndef plot_accuracies(history):\n    accuracies = [x['validation_accuracy'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.grid(True);  # Add grid lines\n\nplot_accuracies(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['validation_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n\nplot_losses(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\nxb = img.unsqueeze(0).to(device)\n\n\n\n\nCode\n# Test with individual Images\ndef predict_image(img, model, classes):\n  # convert to a batch of 1\n  xb = img.unsqueeze(0).to(device) # It returns a new tensor with a dimension of size one inserted at the specified position dim. --&gt; https://www.geeksforgeeks.org/how-to-squeeze-and-unsqueeze-a-tensor-in-pytorch/\n  # Get the prediction from the model\n  yb = model(xb)\n  # pick index with highest probability\n  _, preds = torch.max(yb, dim=1)\n  # Retreive the class label\n  return classes[preds[0].item()]\n\n\n\n\nCode\ndef show_image_prediction(img, label):\n  plt.imshow(img.permute(1,2,0)) # Returns a view of the original tensor input with its dimensions permuted.--&gt;https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute\n  pred = predict_image(img, model, dataset.classes)\n  print(\"Target:\", dataset.classes[label])\n  print('Prediction:', pred)\n\n\n\n\nCode\nshow_image_prediction(*validation_dataset[23])\n\n\nTarget: Bean\nPrediction: Bean\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg, _ = validation_dataset[23]\nplt.imshow(img.permute(1,2,0))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndummy_input = torch.randn(1, 3, 64, 64).to(device)\ninput_names = [ \"actual_input\" ]\noutput_names = [ \"output\" ]\n\n\n\n\nCode\n!pip install onnx\n\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n\nCollecting onnx\n\n  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 86.8 MB/s eta 0:00:00\n\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\n\nRequirement already satisfied: protobuf&gt;=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n\nRequirement already satisfied: typing-extensions&gt;=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\n\nInstalling collected packages: onnx\n\nSuccessfully installed onnx-1.14.0\n\n\n\n\n\n\nCode\ntorch.onnx.export(model,\n                 dummy_input,\n                 \"ResNet9.onnx\",\n                 verbose=False,\n                 export_params=True,\n                 )\n\n\n============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n\n\n\n\nCode\n!pip install onnxruntime -q\n\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.9 MB ? eta -:--:--\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 5.8/5.9 MB 174.3 MB/s eta 0:00:01\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 93.9 MB/s eta 0:00:00\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 5.6 MB/s eta 0:00:00\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 11.2 MB/s eta 0:00:00\n\n\n\n\n\n\n\nCode\nimport onnxruntime\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Load the ONNX model\nmodel_path = 'ResNet9.onnx'\nsession = onnxruntime.InferenceSession(model_path)\n\n# Define the image transformation function\ndef image_transformation(img):\n    # Open the image\n    image = Image.open(img)\n\n    # Define the transformations\n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor()\n    ])\n\n    # Apply the transformations to the image\n    transformed_image = transform(image)\n\n    return transformed_image\n\n# Define the class labels\nclass_labels = ['Cucumber', 'Capsicum', 'Papaya', 'Tomato', 'Cabbage', 'Pumpkin', 'Bitter_Gourd',\n                'Radish', 'Broccoli', 'Cauliflower', 'Bean', 'Carrot', 'Bottle_Gourd', 'Potato', 'Brinjal']\n\n# Define the function for inference and displaying the image\ndef infer_and_display_image(image_path):\n    # Perform inference\n    input_image = image_transformation(image_path)\n    input_tensor = np.expand_dims(input_image, axis=0)\n\n    # Run inference\n    input_name = session.get_inputs()[0].name\n    output_name = session.get_outputs()[0].name\n    output = session.run([output_name], {input_name: input_tensor})\n\n    # Postprocess the output\n    output = output[0]\n    predicted_class_index = np.argmax(output)\n    predicted_class = class_labels[predicted_class_index]\n\n    # Display the image\n    image = Image.open(image_path)\n    plt.imshow(image)\n\n    # Print the predicted class\n    print(\"Predicted class:\", predicted_class)\n\n\n\n\n\n\nCode\n# Call the function with the image path\nimage_path = '/content/vegetable-image-dataset/Vegetable Images/validation/Carrot/1202.jpg'\ninfer_and_display_image(image_path)\n\n\nPredicted class: Carrot\n\n\n\n\n\n\n\n\n\n\n\nCode\nimage_path = '/content/vegetable-image-dataset/Vegetable Images/validation/Pumpkin/1209.jpg'\ninfer_and_display_image(image_path)\n\n\nPredicted class: Pumpkin"
  },
  {
    "objectID": "projects/Brand Detection/Brand_Detection.html",
    "href": "projects/Brand Detection/Brand_Detection.html",
    "title": "Brand Detection",
    "section": "",
    "text": "Brand Detection\nVisual content, such as videos and images, plays a significant role in modern-day marketing. Traditionally, brands have had to pay content creators to feature their brand logo in their content. However, marketers can now leverage ML-powered computer vision to identify and recognize their products in various forms of content, including videos and images. This technology enables marketers to extract valuable insights from the content and understand the audience’s behaviour better. With this understanding, brands can improve their advertising strategies and achieve higher ROI by targeting their audience more effectively and personalizing their messaging. The potential benefits of ML-powered computer vision in marketing make it an exciting area of exploration for brands and marketers.\n\n\n\nimage\n\n\n\n\nInstruction to train the model in Google Colab\n!rm -r /content/sample_data; # remove the sample directory from google colab\n!git clone https://github.com/07Sada/brand.git # clone the repository\n# change the directory\n%cd /content/brand \n# install the requirements\n%pip install -r /content/brand/requirements.txt -q\n# initiate the training\nfrom BrandRecognition.pipeline.training_pipeline import TrainPipeline\nobj = TrainPipeline()\nobj.run_pipeline()\n\n\nScreenshots\n\n\n\nimage\n\n\n\n\nDemo\n\n\n\nPoject_video_#01\n\n\n\n\n\nPoject_video_#02"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Accessing Ollama in Free Google Colab Session\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nDay 1 Web Scraping and AI Summarization\n\n\n\n\n\n\n\n\nMay 3, 2025\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Actions Continuous Integration CI\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub Actions Introduction\n\n\n\n\n\n\n\n\nDec 2, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Language Processing(NLP)\n\n\n\n\n\n\n\n\nNov 27, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nChi Square Test\n\n\n\n\n\n\n\n\nNov 21, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nSupport Vector Machines\n\n\n\n\n\n\n\n\nNov 20, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nSupervised Learning Algorithms\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nUnsupervised Learning Algorithms\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Docker\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection Techniques in Machine Learning\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Algorithm\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nSadashiv\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sadashiv Nandanikar",
    "section": "",
    "text": "Data Scientist @ Maruti Suzuki India Limited.\nExperienced Data Scientist | Machine Learning Specialist with a proven track record of driving business growth through data-driven insights, advanced analytics, and Python expertise."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "",
    "text": "Feature selection is a way of selecting the subset of the most relevant features from the original features set by removing the redundant, irrelevant, or noisy features.\nLet’s first understand some basics of feature selection."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#what-is-feature-selection",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#what-is-feature-selection",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "What is Feature Selection?",
    "text": "What is Feature Selection?\nA feature is an attribute that has an impact on a problem or is useful for the problem, and choosing the important features for the model is known as feature selection.\nEach machine learning process depends on feature engineering, which mainly contains two processes; which are Feature Selection and Feature Extraction. Although feature selection and extraction processes may have the same objective, both are completely different from each other.\n\nThe main difference between them is that feature selection is about selecting the subset of the original feature set, whereas feature extraction creates new features.\nFeature selection is a way of reducing the input variable for the model by using only relevant data in order to reduce overfitting in the model.\n\nSo, we can define feature Selection as, “It is a process of automatically or manually selecting the subset of most appropriate and relevant features to be used in model building.” Feature selection is performed by either including the important features or excluding the irrelevant features in the dataset without changing them."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#need-for-feature-selection",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#need-for-feature-selection",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "Need for Feature Selection",
    "text": "Need for Feature Selection\n\nAs we know, in machine learning, it is necessary to provide a pre-processed and good input dataset in order to get better outcomes. We collect a huge amount of data to train our model and help it to learn better.\nGenerally, the dataset consists of noisy data, irrelevant data, and some part of useful data.\nMoreover, the huge amount of data also slows down the training process of the model, and with noise and irrelevant data, the model may not predict and perform well.\nSo, it is very necessary to remove such noises and less-important data from the dataset and to do this, and Feature selection techniques are used.\n\nSelecting the best features helps the model to perform well. For example, Suppose we want to create a model that automatically decides which car should be crushed for a spare part, and to do this, we have a dataset. This dataset contains a Model of the car, Year, Owner’s name, Miles. So, in this dataset, the name of the owner does not contribute to the model performance as it does not decide if the car should be crushed or not, so we can remove this column and select the rest of the features(column) for the model building.\nBelow are some benefits of using feature selection in machine learning: - It helps in avoiding the [[Curse of Dimensionality]]. - It helps in the simplification of the model so that it can be easily interpreted by the researchers. - It reduces the training time. - It reduces overfitting hence enhance the generalization."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#feature-selection-techniques",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#feature-selection-techniques",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "Feature Selection Techniques",
    "text": "Feature Selection Techniques\nThere are mainly two types of Feature Selection techniques, which are: - Supervised Feature Selection technique Supervised Feature selection techniques consider the target variable and can be used for the labelled dataset. - Unsupervised Feature Selection technique Unsupervised Feature selection techniques ignore the target variable and can be used for the unlabeled dataset.\n\n\n\nFeature_Selection_Techniques_Drawing\n\n\nThere are mainly three techniques under supervised feature Selection:\n\n1. Wrapper Methods\nIn wrapper methodology, selection of features is done by considering it as a search problem, in which different combinations are made, evaluated, and compared with other combinations. It trains the algorithm by using the subset of features iteratively.\n\nOn the basis of the output of the model, features are added or subtracted, and with this feature set, the model has trained again.\nSome techniques of wrapper methods are:\n\nForward selection: Forward selection is an iterative process, which begins with an empty set of features. After each iteration, it keeps adding on a feature and evaluates the performance to check whether it is improving the performance or not. The process continues until the addition of a new variable/feature does not improve the performance of the model. ^71dca8\nBackward elimination: Backward elimination is also an iterative approach, but it is the opposite of forward selection. This technique begins the process by considering all the features and removes the least significant feature. This elimination process continues until removing the features does not improve the performance of the model.\nExhaustive Feature Selection: Exhaustive feature selection is one of the best feature selection methods, which evaluates each feature set as brute-force. It means this method tries & make each possible combination of features and return the best performing feature set.\nRecursive Feature Elimination: Recursive feature elimination is a recursive greedy optimization approach, where features are selected by recursively taking a smaller and smaller subset of features. Now, an estimator is trained with each set of features, and the importance of each feature is determined using coef_attribute or through a _feature_importances_attribute.\n\n\n\n2. Filter Methods\nIn Filter Method, features are selected on the basis of statistics measures. This method does not depend on the learning algorithm and chooses the features as a pre-processing step.\nThe filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking.\nThe advantage of using filter methods is that it needs low computational time and does not overfit the data.\n\nSome common techniques of Filter methods are as follows:\n\nInformation Gain: Information gain determines the reduction in entropy while transforming the dataset. It can be used as a feature selection technique by calculating the information gain of each variable with respect to the target variable.\nChi-square Test: Chi-square test is a technique to determine the relationship between the categorical variables. The chi-square value is calculated between each feature and the target variable, and the desired number of features with the best chi-square value is selected.\nFisher’s Score: Fisher’s score is one of the popular supervised technique of features selection. It returns the rank of the variable on the fisher’s criteria in descending order. Then we can select the variables with a large fisher’s score.\nMissing Value Ratio: The value of the missing value ratio can be used for evaluating the feature set against the threshold value. The formula for obtaining the missing value ratio is the number of missing values in each column divided by the total number of observations. The variable is having more than the threshold value can be dropped.\n\\[\\text{Missing Value Ratio} = \\frac{\\text{Number of Missing Values} \\times 100}{\\text{Total Number of Observations}}\n  \\]\n\n\n\n3. Embedded Methods\nEmbedded methods combined the advantages of both filter and wrapper methods by considering the interaction of features along with low computational cost. These are fast processing methods similar to the filter method but more accurate than the filter method.\n\nThese methods are also iterative, which evaluates each iteration, and optimally finds the most important features that contribute the most to training in a particular iteration. Some techniques of embedded methods are:\n\nRegularization: Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model. This penalty term is added to the coefficients; hence it shrinks some coefficients to zero. Those features with zero coefficients can be removed from the dataset. The types of regularization techniques are L1 Regularization (Lasso Regularization) or Elastic Nets (L1 and L2 regularization).\nRandom Forest Importance: Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features. Here, feature importance specifies which feature has more importance in model building or has a great impact on the target variable. Random Forest is such a tree-based method, which is a type of bagging algorithm that aggregates a different number of decision trees. It automatically ranks the nodes by their performance or decrease in the impurity (Gini impurity) over all the trees. Nodes are arranged as per the impurity values, and thus it allows to pruning of trees below a specific node. The remaining nodes create a subset of the most important features."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#how-to-choose-a-feature-selection-method",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#how-to-choose-a-feature-selection-method",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "How to choose a Feature Selection Method?",
    "text": "How to choose a Feature Selection Method?\nFor machine learning engineers, it is very important to understand that which feature selection method will work properly for their model. The more we know the datatypes of variables, the easier it is to choose the appropriate statistical measure for feature selection.\n\n\n\nFeature_Selection_Flow_Chart\n\n\nTo know this, we need to first identify the type of input and output variables. In machine learning, variables are of mainly two types:\n\nNumerical Variables: Variable with continuous values such as integer, float\nCategorical Variables: Variables with categorical values such as Boolean, ordinal, nominals.\n\nBelow are some univariate statistical measures, which can be used for filter-based feature selection:\n\nNumerical Input, Numerical Output: Numerical Input variables are used for predictive regression modelling. The common method to be used for such a case is the Correlation coefficient.\n\nPearson’s correlation coefficient (For linear Correlation).\nSpearman’s rank coefficient (for non-linear correlation).\n\n Numerical Input, Categorical Output: Numerical Input with categorical output is the case for classification predictive modelling problems. In this case, also, correlation-based techniques should be used, but with categorical output.\n\nANOVA correlation coefficient (linear).\nKendall’s rank coefficient (nonlinear).\n\nCategorical Input, Numerical Output: This is the case of regression predictive modelling with categorical input. It is a different example of a regression problem. We can use the same measures as discussed in the above case but in reverse order.\nCategorical Input, Categorical Output: This is a case of classification predictive modelling with categorical Input variables.\nThe commonly used technique for such a case is Chi-Squared Test. We can also use Information gain in this case.\n\n\n\n\n\n\n\n\n\nInput Variable\nOutput Variable\nFeature Selection technique\n\n\n\n\nNumerical\nNumerical\n- Pearson’s correlation coefficient (For linear Correlation).- Pearson’s correlation coefficient (For linear Correlation)\n\n\nNumerical\nCategorical\n- ANOVA correlation coefficient (linear).  - Kendall’s rank coefficient (nonlinear).\n\n\nCategorical\nNumerical\n- Kendall’s rank coefficient (linear).  - ANOVA correlation coefficient (nonlinear).\n\n\nCategorical\nCategorical\n- Chi-Squared test (contingency tables).  - Mutual Information."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html",
    "title": "Introduction to Docker",
    "section": "",
    "text": "Docker is a platform designed to help developers build, share, and run container applications."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#what-is-docker",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#what-is-docker",
    "title": "Introduction to Docker",
    "section": "",
    "text": "Docker is a platform designed to help developers build, share, and run container applications."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#why-do-we-need-dockers",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#why-do-we-need-dockers",
    "title": "Introduction to Docker",
    "section": "Why do we need Dockers?",
    "text": "Why do we need Dockers?\n\nConsistency Across Environments\n\n\nProblem: Applications often behave differently in development, testing, and production environments due to variations in configurations, dependencies, and infrastructure.\nSolution: Docker containers encapsulate all the necessary components, ensuring the application runs consistently across all environments.\n\n\n\nIsolation\n\n\nProblem: Running multiple applications on the same host can lead to conflicts, such as dependency clashes or resource contention.\nSolution: Docker provides isolated environments for each application, preventing interference and ensuring stable performance.\n\n\n\nScalability\n\n\nProblem: Scaling applications to handle increased load can be challenging, requiring manual intervention and configuration.\nSolution: Docker makes it easy to scale applications horizontally by running multiple container instances, allowing for quick and efficient scaling."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#how-exactly-docker-is-used",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#how-exactly-docker-is-used",
    "title": "Introduction to Docker",
    "section": "How exactly Docker is used?",
    "text": "How exactly Docker is used?\n\n\n\nImage credit: Geeksforgeeks\n\n\n\nDocker Engine\nDocker Engine is the core component of the Docker platform, responsible for creating, running, and managing Docker containers. It serves as the runtime that powers Docker’s containerization capabilities. Here’s an in-depth look at the Docker Engine:\n\n\nComponents of Docker Engine\n\n1. Docker Daemon (dockerd):\n\nFunction : The Docker daemon is the background service running on the host machine. It manages Docker objects such as images, containers, networks, and volumes.\nInteraction : It listens for Docker API requests and processes them, handling container lifecycle operations (start, stop, restart, etc.).\n\n\n\n2. Docker CLI (docker):\n\nFunction : The Docker Command Line Interface (CLI) is the tool that users interact with to communicate with the Docker daemon.\nUsage : Users run Docker commands through the CLI to perform tasks like building images, running containers, and managing Docker resources.\n\n\n\n3. REST API :\n\nFunction : The Docker REST API allows communication between the Docker CLI and the Docker daemon. It also enables programmatic interaction with Docker.\nUsage : Developers can use the API to automate Docker operations or integrate Docker functionality into their applications.\n\n\n\n\nDocker Image\nA Docker image is a lightweight, stand-alone, and executable software package that includes everything needed to run a piece of software, such as the code, runtime, libraries, environment variables, and configuration files. Images are used to create Docker containers, which are instances of these images.\n\nComponents of a Docker Image\n\nBase Image : The starting point for building an image. It could be a minimal OS image like alpine, a full-fledged OS like ubuntu, or even another application image like python or node.\nApplication Code : The actual code and files necessary for the application to run.\nDependencies : Libraries, frameworks, and packages required by the application.\nMetadata : Information about the image, such as environment variables, labels, and exposed ports.\n\n\n\nDocker Image Lifecycle\n\nCreation : Images are created using the docker build command, which processes the instructions in a Dockerfile to create the image layers.\n\nStorage : Images are stored locally on the host machine. They can also be pushed to and pulled from Docker registries like Docker Hub, AWS ECR, or Google Container Registry.\n\nDistribution : Images can be shared by pushing them to a Docker registry, allowing others to pull and use the same image.\n\nExecution : Images are executed by running containers, which are instances of these images.\n\n\n\nDockerfile\nA Dockerfile is a text file that contains a series of instructions used to build a Docker image. Each instruction in a Dockerfile creates a layer in the image, allowing for efficient image creation and reuse of layers. Dockerfiles are used to automate the image creation process, ensuring consistency and reproducibility.\n\nKey Components of a Dockerfile\n\nBase Image (FROM) : Specifies the starting point for the image, which could be a minimal operating system, a specific version of a language runtime, or another image.\nExample: FROM ubuntu:20.04\nLabels (LABEL) : Adds metadata to the image, such as version, description, or maintainer.\nExample: LABEL version=\"1.0\" description=\"My application\"\nRun Commands (RUN) : Executes commands in the image during the build process, typically used to install software packages. Example: docker RUN apt-get update && apt-get install -y python3\nCopy Files (COPY): Copies files or directories from the host system to the image.\nExample: COPY . /app\nEnvironment Variables (ENV) : Sets environment variables in the image.\nExample: ENV PATH /app/bin:$PATH\nWork Directory (WORKDIR) : Sets the working directory for subsequent instructions.\nExample: WORKDIR /app\nExpose Ports (EXPOSE) : Informs Docker that the container listens on specified network ports.\nExample: EXPOSE 8080\nCommand (CMD) : Provides a default command to run when the container starts.\nExample: CMD [\"python\", \"app.py\"]\nVolume (VOLUME) : Creates a mount point with a specified path and marks it as holding externally mounted volumes from the host or other containers.\nExample: VOLUME [\"/data\"]\nArguments (ARG) : Defines build-time variables.\nExample: ARG VERSION=1.0\n\n# Use an Official python runtime as a base image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app \n\n# Copy the current directory contents into the container at /app \nCOPY . /app \n\n# Install the needed packages specified in requirements.txt\nRUN pip install -no-cache-dir -r requirements.txt \n\n# Make port 80 available to the world outside this container \nEXPOSE 80\n\n# Define enviroment vaiable\nENV NAME world\n\n# Run app.py when the container launches \nCMD [\"python\", \"app.py\"]\n\n\n\nDocker Container\nA Docker container is a lightweight, portable, and isolated environment that encapsulates an application and its dependencies, allowing it to run consistently across different computing environments. Containers are created from Docker images, which are immutable and contain all the necessary components for the application to run.\n\n\n\n\nRegistry\nA Docker registry is a service that stores and distributes Docker images. It acts as a repository where users can push, pull, and manage Docker images. Docker Hub is the most well-known public registry, but private registries can also be set up to securely store and manage images within an organization.\n\nKey Components of a Docker Registry:\n\nRepositories : A repository is a collection of related Docker images, typically different versions of the same application. Each repository can hold multiple tags, representing different versions of an image.\nTags : Tags are used to version images within a repository.\nFor example, myapp:1.0, myapp:2.0, and myapp:latest are tags for different versions of the myapp image.\n\n\n\nTypes of Docker Registries\n\nDocker Hub:\n\n\nDescription : The default public registry provided by Docker, which hosts a vast number of public images and also supports private repositories.\n\nURL : hub.docker.com\n\nUse Case : Publicly sharing images and accessing a large collection of pre-built images from the community and official repositories.\n\nPrivate Registries :\n\n\nDescription : Custom registries set up by organizations to securely store and manage their own Docker images.\n\nUse Case : Ensuring security and control over image distribution within an organization.\n\nThird-Party Registries :\n\n\nExamples : Amazon Elastic Container Registry (ECR), Google Container Registry (GCR), Azure Container Registry (ACR).\n\nUse Case : Integrating with cloud platforms for seamless deployment and management of images within cloud infrastructure.\n\n\n\n\nBenefits of Using Docker Registries\n1. Centralized Image Management\n\nRegistries provide a centralized location to store and manage Docker images, making it easier to organize and distribute them.\n2. Version Control :\n\nUsing tags, registries allow version control of images, enabling users to easily roll back to previous versions if needed.\n3. Collaboration:\n\nPublic registries like Docker Hub facilitate collaboration by allowing users to share images with the community or within teams.\n4. Security :\n\nPrivate registries ensure that sensitive images are stored securely and access is controlled within an organization.\n5. Integration with CI/CD :\n\nRegistries integrate seamlessly with CI/CD pipelines, automating the process of building, storing, and deploying Docker images.\n\n\n\nUse-cases\n\n1. Microservices Architecture\n\n\nDescription : Microservices break down applications into smaller, independent services, each running in its own container.\nBenefits : Simplifies deployment, scaling, and maintenance. Each service can be developed, updated, and deployed independently.\n\n\n\n2. Continuous Integration and Continuous Deployment (CI/CD)\n\n\nDescription : Docker ensures a consistent environment from development through testing to production.\nBenefits : Streamlines the CI/CD pipeline, reduces discrepancies between environments, and speeds up testing and deployment processes.\n\n\n\n3. Cloud Migration\n\n\nDescription : Containerizing applications to move them to the cloud.\nBenefits : Simplifies the migration process, allows applications to run consistently across different cloud providers, and optimizes resource usage.\n\n\n\n4. Scalable Web Applications\n\n\nDescription : Deploying web applications in containers for easy scaling.\nBenefits : Simplifies scaling up or down based on traffic, ensures consistent deployment, and enhances resource utilization.\n\n\n\n5. Testing and QA\n\n\nDescription : Creating consistent environments for testing applications.\nBenefits : Ensures tests are run in environments identical to production, speeds up the setup of test environments, and facilitates automated testing.\n\n\n\n6. Machine Learning and AI\n\n\nDescription : Deploying machine learning models and AI applications in containers.\nBenefits : Ensures consistency in the runtime environment, simplifies scaling of model training and inference, and facilitates collaboration and reproducibility.\n\n\n\n7. API Development and Deployment\n\n\nDescription : Developing and deploying APIs in containers.\nBenefits : Ensures APIs run consistently across environments, simplifies scaling, and improves deployment speed and reliability."
  },
  {
    "objectID": "blogs/Chi_Square_Test/Chi Square Test.html",
    "href": "blogs/Chi_Square_Test/Chi Square Test.html",
    "title": "Chi Square Test",
    "section": "",
    "text": "The chi-square test is a statistical test used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categorical variables. It is commonly used to determine whether there is a significant association between two categorical variables\n\n\n\n\nExample of how the chi-square test can be used to determine whether there is a significant association between two categorical variables:\nSuppose we are interested in studying the relationship between diet and heart disease. We collect data on 100 people and record their diet (healthy or unhealthy) and whether they have heart disease (yes or no). The data looks like this:\n\n\n\nDiet\nHeart Disease\nHeart Disease\n\n\n\n\nHealthy\nYes\n20\n\n\nHealthy\nNo\n10\n\n\nUnhealthy\nYes\n30\n\n\nUnhealthy\nNo\n40\n\n\n\nWe can use the chi-square test to determine whether there is a significant association between diet and heart disease. Our null hypothesis is that there is no significant association between the two variables.\nTo perform the chi-square test, we first need to calculate the expected frequencies for each cell in the table. The expected frequency for a cell is the product of the row total and the column total for that cell, divided by the overall total.\nFor example, the expected frequency for the cell “Healthy, No Heart Disease” is: \\[(20 + 10) * (30 + 40) / 100 = 25\\] We can calculate the expected frequencies for all cells in the table as follows:\n\n\n\nDiet\nHeart Disease\nExpected Frequency\n\n\n\n\nHealthy\nYes\n25\n\n\nHealthy\nNo\n25\n\n\nUnhealthy\nYes\n50\n\n\nUnhealthy\nNo\n50\n\n\n\nNext, we calculate the chi-square statistic as follows:\n\\(\\Large\\sum\\left(\\frac{(Observed\\ Frequency - Expected\\ Frequency)^2}{Expected\\ Frequency}\\right)\\)\n\\(= \\Large\\frac{(20 - 25)^2}{25} + \\frac{(10 - 25)^2}{25} + \\frac{(30 - 50)^2}{50} + \\frac{(40 - 50)^2}{50} = 7.20\\)\nFinally, we compare the chi-square statistic to a critical value from a chi-square distribution table or by using a computer program. If the chi-square statistic is greater than the critical value, we reject the null hypothesis and conclude that there is a significant association between diet and heart disease. If the chi-square statistic is less than the critical value, we fail to reject the null hypothesis and conclude that there is no significant association between the two variables.\n\n\n\n\nThe chi-square test is often used in machine learning to evaluate the performance of a classification model. It is used to determine whether the predicted class labels from the model are significantly different from the true class labels. This can help you ensure that your model is making fair and accurate predictions and improve its overall performance.\nThe chi-square test can also be used to select the best features for a machine learning model. By measuring the correlation between each feature and the target variable, you can identify the most relevant and predictive features for your model. This can help improve the performance of your model by using only the most relevant and predictive features.\n\n\n\n\nFor example, suppose you have trained a classification model to predict whether a customer will churn (i.e., stop using your service). You can use the chi-square test to compare the predicted churn rates for different groups of customers (e.g., by age, gender, location) to the observed churn rates. If the predicted churn rates are significantly different from the observed churn rates, this may indicate that the model is making biased or inaccurate predictions.\nThe chi-square test can also be used to select the best features for a machine learning model. By measuring the correlation between each feature and the target variable, you can identify the most relevant and predictive features for your model.\n\n\n\n\n\n\nReference Article Chi-Square Test for Feature Selection in Machine learning | by sampath kumar gajawada | Towards Data Science\nPractical Implementation [[Feature_Selection_004_Chi-Square Test#Practical Implementation]]\n\n\n\n\n\n1. What is the chi-square test and how is it used in machine learning?\nThe chi-square test is a statistical test that is used to determine whether there is a significant association between two categorical variables. In machine learning, it is often used to evaluate the performance of a classification model and to select the best features for a model.\n2. When would you use the chi-square test in machine learning?\nThe chi-square test is typically used in machine learning when you want to evaluate the performance of a classification model or when you want to select the best features for a model. It is particularly useful when the data do not meet the assumptions of other statistical tests, such as normality or equal variance.\n3. How do you interpret the results of a chi-square test in machine learning?\nThe chi-square test returns a p-value, which indicates the probability of obtaining the observed results if the null hypothesis (i.e., the variables are independent) is true. If the p-value is less than a predetermined threshold (e.g., 0.05), you can reject the null hypothesis and conclude that there is a significant association between the variables. If the p-value is greater than the threshold, you cannot reject the null hypothesis and conclude that there is no significant association.\n4. How can you use the chi-square test to select the best features for a machine learning model?\nThe chi-square test can be used to select the best features for a machine learning model by evaluating the statistical significance of the relationship between each feature and the target variable. Features with a significant relationship to the target variable are more likely to be useful for predicting the target variable and are therefore more likely to be selected.\nHere are the steps on how to use the chi-square test to select the best features for a machine learning model:\n\nGather data. The first step is to gather data that includes the features you want to evaluate and the target variable.\nPrepare the data. The data should be prepared in a way that is compatible with the chi-square test. This may involve converting categorical variables to binary variables and removing features with missing values.\nCalculate the chi-square statistic. The chi-square statistic is calculated by comparing the observed frequencies of each feature with the expected frequencies if there was no relationship between the feature and the target variable.\nDetermine the p-value. The p-value is the probability of getting a chi-square statistic as extreme or more extreme than the one we calculated, assuming that there is no relationship between the two variables.\nSelect features. Features with a p-value less than a predetermined significance level are selected. The significance level is typically set to 0.05, which means that there is a 5% chance of selecting a feature that is not actually related to the target variable.\n\nThe chi-square test is a powerful tool that can be used to select the best features for a machine learning model. However, it is important to note that the chi-square test is only a statistical test and does not guarantee that the selected features will be useful for predicting the target variable. It is important to evaluate the performance of the selected features on a machine learning model to ensure that they are actually useful.\n5. How does the chi-square test compare to other statistical tests (e.g., t-test, ANOVA) in terms of its assumptions and power?\nThe chi-square test is a nonparametric test that does not assume a specific distribution or shape of the data. It is often used to evaluate the performance of a classification model or to select the best features for a machine learning model.\nIn comparison to other statistical tests, such as the t-test and ANOVA, the chi-square test has different assumptions and power. The t-test and ANOVA are parametric tests that assume that the data are normally distributed and have equal variances, respectively.These tests may be more powerful than the chi-square test when these assumptions are met, but may be less robust when the assumptions are violated.\nIn summary, the chi-square test is a nonparametric test that does not assume a specific distribution or shape of the data, and may be less powerful but more robust than parametric tests such as the t-test and ANOVA.\n6. What are some advantages and limitations of using the chi-square test in machine learning?\n\nSome advantages of using the chi-square test in machine learning include:\n\nIt is a widely used and well-established statistical test.\nIt is relatively easy to understandand implement.\nIt can be used to evaluate the performance of a classification model or select the best features for a machine learning model.\nIt is generally more robust than parametric tests, as it does not assume a specific distribution or shape of the data.\n\nHowever, there are also some limitations of using the chi-square test in machine learning:\n\nIt may be less powerful than parametric tests, and may not be as sensitive to detecting differences in the data.\nIt is only applicable to categorical data, and cannot be used with continuous or ordinal data.\nIt requires a large sample size to be effective, and may not be reliable with small sample sizes.\nIt may not be suitable for comparing more than two groups or categories."
  },
  {
    "objectID": "blogs/Chi_Square_Test/Chi Square Test.html#chi-square-test",
    "href": "blogs/Chi_Square_Test/Chi Square Test.html#chi-square-test",
    "title": "Chi Square Test",
    "section": "",
    "text": "The chi-square test is a statistical test used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categorical variables. It is commonly used to determine whether there is a significant association between two categorical variables\n\n\n\n\nExample of how the chi-square test can be used to determine whether there is a significant association between two categorical variables:\nSuppose we are interested in studying the relationship between diet and heart disease. We collect data on 100 people and record their diet (healthy or unhealthy) and whether they have heart disease (yes or no). The data looks like this:\n\n\n\nDiet\nHeart Disease\nHeart Disease\n\n\n\n\nHealthy\nYes\n20\n\n\nHealthy\nNo\n10\n\n\nUnhealthy\nYes\n30\n\n\nUnhealthy\nNo\n40\n\n\n\nWe can use the chi-square test to determine whether there is a significant association between diet and heart disease. Our null hypothesis is that there is no significant association between the two variables.\nTo perform the chi-square test, we first need to calculate the expected frequencies for each cell in the table. The expected frequency for a cell is the product of the row total and the column total for that cell, divided by the overall total.\nFor example, the expected frequency for the cell “Healthy, No Heart Disease” is: \\[(20 + 10) * (30 + 40) / 100 = 25\\] We can calculate the expected frequencies for all cells in the table as follows:\n\n\n\nDiet\nHeart Disease\nExpected Frequency\n\n\n\n\nHealthy\nYes\n25\n\n\nHealthy\nNo\n25\n\n\nUnhealthy\nYes\n50\n\n\nUnhealthy\nNo\n50\n\n\n\nNext, we calculate the chi-square statistic as follows:\n\\(\\Large\\sum\\left(\\frac{(Observed\\ Frequency - Expected\\ Frequency)^2}{Expected\\ Frequency}\\right)\\)\n\\(= \\Large\\frac{(20 - 25)^2}{25} + \\frac{(10 - 25)^2}{25} + \\frac{(30 - 50)^2}{50} + \\frac{(40 - 50)^2}{50} = 7.20\\)\nFinally, we compare the chi-square statistic to a critical value from a chi-square distribution table or by using a computer program. If the chi-square statistic is greater than the critical value, we reject the null hypothesis and conclude that there is a significant association between diet and heart disease. If the chi-square statistic is less than the critical value, we fail to reject the null hypothesis and conclude that there is no significant association between the two variables.\n\n\n\n\nThe chi-square test is often used in machine learning to evaluate the performance of a classification model. It is used to determine whether the predicted class labels from the model are significantly different from the true class labels. This can help you ensure that your model is making fair and accurate predictions and improve its overall performance.\nThe chi-square test can also be used to select the best features for a machine learning model. By measuring the correlation between each feature and the target variable, you can identify the most relevant and predictive features for your model. This can help improve the performance of your model by using only the most relevant and predictive features.\n\n\n\n\nFor example, suppose you have trained a classification model to predict whether a customer will churn (i.e., stop using your service). You can use the chi-square test to compare the predicted churn rates for different groups of customers (e.g., by age, gender, location) to the observed churn rates. If the predicted churn rates are significantly different from the observed churn rates, this may indicate that the model is making biased or inaccurate predictions.\nThe chi-square test can also be used to select the best features for a machine learning model. By measuring the correlation between each feature and the target variable, you can identify the most relevant and predictive features for your model.\n\n\n\n\n\n\nReference Article Chi-Square Test for Feature Selection in Machine learning | by sampath kumar gajawada | Towards Data Science\nPractical Implementation [[Feature_Selection_004_Chi-Square Test#Practical Implementation]]\n\n\n\n\n\n1. What is the chi-square test and how is it used in machine learning?\nThe chi-square test is a statistical test that is used to determine whether there is a significant association between two categorical variables. In machine learning, it is often used to evaluate the performance of a classification model and to select the best features for a model.\n2. When would you use the chi-square test in machine learning?\nThe chi-square test is typically used in machine learning when you want to evaluate the performance of a classification model or when you want to select the best features for a model. It is particularly useful when the data do not meet the assumptions of other statistical tests, such as normality or equal variance.\n3. How do you interpret the results of a chi-square test in machine learning?\nThe chi-square test returns a p-value, which indicates the probability of obtaining the observed results if the null hypothesis (i.e., the variables are independent) is true. If the p-value is less than a predetermined threshold (e.g., 0.05), you can reject the null hypothesis and conclude that there is a significant association between the variables. If the p-value is greater than the threshold, you cannot reject the null hypothesis and conclude that there is no significant association.\n4. How can you use the chi-square test to select the best features for a machine learning model?\nThe chi-square test can be used to select the best features for a machine learning model by evaluating the statistical significance of the relationship between each feature and the target variable. Features with a significant relationship to the target variable are more likely to be useful for predicting the target variable and are therefore more likely to be selected.\nHere are the steps on how to use the chi-square test to select the best features for a machine learning model:\n\nGather data. The first step is to gather data that includes the features you want to evaluate and the target variable.\nPrepare the data. The data should be prepared in a way that is compatible with the chi-square test. This may involve converting categorical variables to binary variables and removing features with missing values.\nCalculate the chi-square statistic. The chi-square statistic is calculated by comparing the observed frequencies of each feature with the expected frequencies if there was no relationship between the feature and the target variable.\nDetermine the p-value. The p-value is the probability of getting a chi-square statistic as extreme or more extreme than the one we calculated, assuming that there is no relationship between the two variables.\nSelect features. Features with a p-value less than a predetermined significance level are selected. The significance level is typically set to 0.05, which means that there is a 5% chance of selecting a feature that is not actually related to the target variable.\n\nThe chi-square test is a powerful tool that can be used to select the best features for a machine learning model. However, it is important to note that the chi-square test is only a statistical test and does not guarantee that the selected features will be useful for predicting the target variable. It is important to evaluate the performance of the selected features on a machine learning model to ensure that they are actually useful.\n5. How does the chi-square test compare to other statistical tests (e.g., t-test, ANOVA) in terms of its assumptions and power?\nThe chi-square test is a nonparametric test that does not assume a specific distribution or shape of the data. It is often used to evaluate the performance of a classification model or to select the best features for a machine learning model.\nIn comparison to other statistical tests, such as the t-test and ANOVA, the chi-square test has different assumptions and power. The t-test and ANOVA are parametric tests that assume that the data are normally distributed and have equal variances, respectively.These tests may be more powerful than the chi-square test when these assumptions are met, but may be less robust when the assumptions are violated.\nIn summary, the chi-square test is a nonparametric test that does not assume a specific distribution or shape of the data, and may be less powerful but more robust than parametric tests such as the t-test and ANOVA.\n6. What are some advantages and limitations of using the chi-square test in machine learning?\n\nSome advantages of using the chi-square test in machine learning include:\n\nIt is a widely used and well-established statistical test.\nIt is relatively easy to understandand implement.\nIt can be used to evaluate the performance of a classification model or select the best features for a machine learning model.\nIt is generally more robust than parametric tests, as it does not assume a specific distribution or shape of the data.\n\nHowever, there are also some limitations of using the chi-square test in machine learning:\n\nIt may be less powerful than parametric tests, and may not be as sensitive to detecting differences in the data.\nIt is only applicable to categorical data, and cannot be used with continuous or ordinal data.\nIt requires a large sample size to be effective, and may not be reliable with small sample sizes.\nIt may not be suitable for comparing more than two groups or categories."
  },
  {
    "objectID": "blogs/Supervised_Learning_Algorithms/Supervised Learning Algorithms.html",
    "href": "blogs/Supervised_Learning_Algorithms/Supervised Learning Algorithms.html",
    "title": "Supervised Learning Algorithms",
    "section": "",
    "text": "Imagine teaching a computer like a teacher instructs a student. In supervised learning, we provide the computer with labeled examples. It’s like showing it the right answers and letting it figure out how to get there.\n\n\n\nsupervised learning\n\n\nWhy Do We Use It?\nSupervised learning helps the computer make predictions, classify things, and make decisions based on past experiences.\nMeet the Superstars!\n\n[[Linear Regression in Machine Learning|Linear Regression]]: Think of it as drawing a straight line through dots on a graph to make predictions, like guessing a house’s price based on its size.\n[[Decision Tree|Decision Trees]]: Imagine playing 20 Questions. It’s like that but for sorting things, such as deciding if an email is spam or not.\nSupport Vector Machines (SVM): Picture separating different groups of data with a clear line. It’s used in things like sorting images or categorizing text.\n\nHow It Works\n\nTraining: We teach the algorithm using a dataset with examples and their correct answers. It learns by finding patterns in this training data.\nTesting: After training, we test it on new, unseen data to see how well it can predict or classify.\n\nIn the Real World\n\nPredicting stock prices based on past trends.\nDeciding if an email is junk or not.\nRecognizing handwritten numbers in ZIP codes.\n\nPros and Cons\n\nPros: It’s widely used and can be very accurate if the training data is good.\nCons: You need labeled data (which can be costly), and it might not work well if the data is messy or the problem is tricky.\n\nTakeaway\nSupervised learning is like training a pet with treats. It’s essential in machine learning, especially when you know what the right answers should be. It’s like the foundation of our machine learning journey!\n\n\nReference Reading\n# Supervised Learning: Algorithms, Examples, and How It Works"
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html",
    "title": "Natural Language Processing(NLP)",
    "section": "",
    "text": "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data."
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#what-is-nlp",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#what-is-nlp",
    "title": "Natural Language Processing(NLP)",
    "section": "",
    "text": "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data."
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#what-is-natural-language",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#what-is-natural-language",
    "title": "Natural Language Processing(NLP)",
    "section": "What is Natural Language",
    "text": "What is Natural Language\nIn neuropsychology, linguistics, and the philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic"
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#real-world-applications",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#real-world-applications",
    "title": "Natural Language Processing(NLP)",
    "section": "Real World Applications",
    "text": "Real World Applications\n\nContextual Advertisements\nNatural Language Processing (NLP) is used for contextual advertisements by analyzing the text content of a webpage or a user’s search query to understand the context and intent behind it. Based on this analysis, relevant ads are shown to the user that match their interests and needs. NLP techniques such as named entity recognition, sentiment analysis, and topic modeling are used to extract relevant information from the text and match it with suitable ads. This helps to provide a more personalized and relevant advertising experience to the user.\nEmail Clients - spam filtering, smart reply\nNLP is used for email clients in two main ways: spam filtering and smart reply. In spam filtering, NLP algorithms are used to analyze the content of incoming emails to determine whether they are legitimate or spam.\nThis is done by analyzing features such as the sender’s email address, the text content, and the presence of certain keywords.\nSmart reply, on the other hand, uses NLP to suggest relevant and personalized responses to emails based on the context and content of the message.\nNLP techniques such as text classification and natural language generation are used for this purpose, resulting in more efficient and effective email communication.\nSocial Media - removing adult content, opinion mining\nNLP is used in social media platforms for two primary purposes: removing adult content and opinion mining. In the first case, NLP algorithms are used to analyze the text, images, and videos posted by users to identify and remove adult content, hate speech, and other inappropriate content. In opinion mining, NLP techniques such as sentiment analysis are used to extract insights from the large volumes of text data generated by social media users. This can help businesses and organizations understand customer opinions and preferences and make informed decisions based on them.\nSearch Engines\nNLP is used extensively in search engines to improve the accuracy and relevance of search results. NLP algorithms are used to analyze the text content of web pages and user queries to understand the underlying meaning and intent.This is done by techniques such as natural language understanding, entity recognition, and text classification. The search engine can then provide more relevant results to the user based on the context and meaning of their query, resulting in a better search experience. NLP is also used for features such as auto-complete, query expansion, and personalized search results.\nChatbots\nNLP is a crucial component of chatbots, which are designed to simulate human conversation and provide automated customer support. NLP algorithms are used to understand user queries and generate appropriate responses. This is done by techniques such as natural language understanding, intent recognition, and named entity recognition. Chatbots can also use sentiment analysis to understand the user’s mood and personalize their response accordingly. By leveraging NLP, chatbots can provide fast and efficient customer support, 24/7, and handle a wide range of queries, leading to enhanced customer satisfaction."
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#common-nlp-tasks",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#common-nlp-tasks",
    "title": "Natural Language Processing(NLP)",
    "section": "Common NLP Tasks",
    "text": "Common NLP Tasks\n\nDocument/Text Classification\nNLP is used for text classification, which involves categorizing large volumes of text into predefined categories or classes. This is useful for tasks such as document classification, spam filtering, and sentiment analysis. NLP algorithms are used to analyze the text content and extract relevant features, such as the presence of certain keywords, the context of the text, and the structure of the document.These features are then used to train machine learning models that can classify new text data into the appropriate categories. NLP-based text classification can be used across a wide range of industries, including finance, healthcare, and e-commerce, to automate processes and improve efficiency.\n\n\n\n\n\n\nNote\n\n\n\nWe do this by cleaning up the words, putting them in order, and showing the computer examples of what each group should look like. Then, the computer can sort new words into the right group based on what it learned before.\n\n\nSentiment analysis\nNLP is used for sentiment analysis, which involves analyzing text data to determine the emotional tone or sentiment expressed by the author.\nSentiment analysis is useful in many applications, such as understanding customer feedback, monitoring social media sentiment, and predicting stock market trends.\nNLP techniques such as natural language understanding and machine learning algorithms are used to identify sentiment-bearing words and phrases and classify them as positive, negative, or neutral.\nSentiment analysis can provide valuable insights into customer attitudes and opinions, allowing businesses to make data-driven decisions and improve their products and services.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to read and understand the feelings people express in their words using NLP. It’s like teaching the computer to know whether someone is happy or sad based on the words they use. This helps us understand how people feel about things like movies, products, or events. It’s like asking your friends how they feel about something, but the computer can do it faster and with more words\n\n\nInformation Retrieval\nNLP is used for information retrieval, which involves finding relevant information from large collections of unstructured text data.Information retrieval is useful in many applications, such as search engines, chatbots, and question-answering systems. NLP techniques such as natural language understanding, text classification, and entity recognition are used to extract relevant information and understand the user’s query or intent. This allows the system to retrieve the most relevant and useful information, leading to a better user experience and increased efficiency.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to find the best information for us by reading and understanding lots of text using NLP. It’s like teaching the computer to look through a big book to find the answer to a question we have. The computer can find the information faster than we can and give us the best answer. It’s like having a super smart helper who can find anything we need!\n\n\nParts of Speech Tagging\nNLP is used for Parts of Speech (POS) tagging, which involves labeling each word in a sentence with its corresponding part of speech, such as noun, verb, adjective, etc. POS tagging is useful in many NLP applications, such as text-to-speech conversion, machine translation, and information retrieval.NLP techniques such as rule-based tagging and statistical tagging are used to automatically assign POS tags to each word based on its context and other linguistic features. Accurate POS tagging helps improve the accuracy of downstream NLP tasks and is an important step in text analysis and understanding.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to recognize the different types of words in a sentence using NLP. It’s like teaching the computer to know whether a word is a person, a thing, an action, or a description. By doing this, the computer can understand what the sentence is about and what each word is doing. It’s like having a robot assistant that can help us understand sentences better!\n\n\nLanguage Detection And Machine Translation\nNLP is used for language detection and machine translation, which involves identifying the language of a text and translating it into another language. Language detection is useful in many applications, such as text classification, sentiment analysis, and content filtering. Machine translation is useful for cross-language communication and understanding, and can be used in applications such as international business and diplomacy.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to understand and speak different languages using NLP. It’s like having a super-smart robot that can translate languages for us! By teaching the computer different languages and their grammar, it can help us understand what someone is saying in a different language, and even help us talk to them back in their own language. It’s like having a personal language tutor at our fingertips!\n\n\nConversational Agents\nNLP is used for conversational agents, which are computer programs designed to simulate human-like conversations with users. Conversational agents, also known as chatbots or virtual assistants, are used in many applications, such as customer support, personal assistants, and language learning. NLP techniques such as natural language understanding and generation, dialogue management, and sentiment analysis are used to enable conversational agents to understand and respond to user queries and generate appropriate responses. The goal is to create conversational agents that are indistinguishable from human agents in their conversational abilities.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to talk to us like humans using NLP. It’s like having a computer friend we can ask questions to and get answers from! By teaching the computer how to understand what we’re saying and how to respond, it can help us with things like finding information, playing games, or just keeping us company. It’s like having a cool virtual assistant we can chat with anytime!\n\n\nKnowledge Graph and QA Systems\nNLP is used for knowledge graph and question-answering (QA) systems, which involve organizing and connecting information from multiple sources to enable efficient information retrieval and answering of user queries. Knowledge graphs represent knowledge as nodes and edges in a graph structure, which can be queried using natural language questions. QA systems use NLP techniques such as entity recognition, relation extraction, and semantic parsing to understand user queries and retrieve relevant information from knowledge graphs or other sources. The goal is to create intelligent systems that can provide accurate and comprehensive answers to user queries.\nClick here for more information\n\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to organize and understand information like we do using NLP. It’s like creating a big brain for the computer! By connecting different pieces of information and understanding what we’re asking, it can help us answer questions or find things we need. It’s like having a really smart friend who knows everything and can help us learn new things too!\n\n\nText Summarization\nNLP is used for text summarization, which involves automatically generating a condensed version of a longer piece of text while preserving its essential information.NLP techniques such as sentence scoring, text classification, and semantic analysis are used to identify and extract the most important sentences or phrases from a document and create a summary. Text summarization is used in various applications such as news articles, academic papers, and legal documents to enable efficient information consumption and understanding. The goal is to create accurate and concise summaries that capture the essence of the original text.\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to read long stories or articles and then make a short summary using NLP. It’s like having a machine that can pick out the most important parts of a story and tell you what it’s about in just a few sentences. It’s really helpful when you don’t have time to read the whole thing, but you still want to know what it’s about!\n\n\nTopic Modelling\nNLP is used for topic modelling, which is a technique that identifies hidden topics or themes in a collection of documents. It involves grouping together similar words and phrases into topics, and then assigning each document a distribution of topics that best represents its content.NLP techniques such as clustering, latent Dirichlet allocation, and word frequency analysis are used to perform topic modelling. Topic modelling is useful in various applications such as information retrieval, recommendation systems, and content analysis. It enables better organization and understanding of large collections of text data by identifying key themes and patterns.\nClick here for more information\n\n\n\n\n\n\n\n\nNote\n\n\n\n💡 We can teach computers to find hidden topics or themes in a bunch of stories or articles using NLP. It’s like having a machine that can group together similar words and ideas to help us understand what the stories are all about. It’s really helpful when we have a lot of stories to read and want to find the important themes quickly!\n\n\nText Generation\nNLP is used for text generation, which involves generating new text that is similar to the style and content of existing text.This can be done using machine learning models that have been trained on large amounts of text data. NLP techniques such as recurrent neural networks and generative adversarial networks are commonly used for text generation. Text generation has various applications such as chatbots, automatic text completion, and content creation.It enables machines to produce human-like text that can be useful in various industries such as marketing and customer service.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n💡 We can use NLP to teach machines to create new text. It’s like teaching a robot how to write stories or messages just like humans do.\n\n\nSpell Checking And Grammar Correction\nNLP is used for spell checking and grammar correction by analyzing written text and comparing it to a database of correct spellings and grammar rules. This involves identifying and correcting spelling errors, as well as suggesting alternative words or phrasings that would improve grammar and overall readability. By using NLP techniques, spell checking and grammar correction tools can quickly and accurately identify errors and suggest corrections, making it easier for writers to produce high-quality written content. \n\n\n\n\n\n\nNote\n\n\n\n💡 Spell checking and grammar correction using NLP means using a computer to help us check if our writing is spelled correctly and if the grammar is right.\n\n\nText parsing\nText parsing is the process of breaking down text into smaller components to better understand its structure and meaning.In natural language processing (NLP), text parsing is used to analyze and extract important information from text, such as identifying the subject, object, and verb in a sentence. It involves using algorithms and models to automatically analyze and categorize different parts of text, allowing for more efficient processing and analysis of large amounts of data. Text parsing is used in a variety of NLP applications, including sentiment analysis, machine translation, and chatbots.\n\n\n\n\n\n\n\nNote\n\n\n\n💡 Text parsing with NLP means understanding the structure and meaning of sentences. Just like how we understand a story by reading it, NLP can help a computer understand what a sentence means and what it’s talking about.\n\n\nSpeech To Text\nSpeech to Text is a popular application of NLP that involves converting spoken language into text. This technology uses techniques like audio signal processing and natural language understanding to accurately transcribe spoken words into written text. Speech to Text is used in various applications like dictation software, automated subtitling for videos, and virtual assistants."
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#approaches-to-nlp",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#approaches-to-nlp",
    "title": "Natural Language Processing(NLP)",
    "section": "Approaches to NLP",
    "text": "Approaches to NLP\nOver the years, Natural Language Processing (NLP) research has seen different approaches, each building upon the previous ones. Some of the major approaches include:\n\nHeuristic methods\nHeuristic methods or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.\n\n\n\n\n\n\n\nNote\n\n\n\n📌 In the early days of NLP, rule-based systems were the primary approach. These methods involved creating hand-crafted rules and heuristics to solve specific language processing problems. While these methods were simple to implement, they were often inflexible and limited by the available knowledge of the language.\n\n\n\n\nExamples\n\nRegular Expression\nRegular regression is a classic statistical approach used in natural language processing (NLP) under the heuristic method. In regular regression, a linear model is trained using a set of features extracted from the text data. These features are typically hand-crafted by domain experts and can include measures of word frequency, length, and syntax. The trained model can then be used for tasks such as sentiment analysis, named entity recognition, and text classification. However, regular regression has some limitations, such as the need for expert feature engineering and its inability to capture complex relationships between words and sentences.\nWordNet\nWordNet is a lexical database of English words, which was developed by researchers at Princeton University. It organizes words into sets of synonyms called “synsets” and describes the semantic relationships between them. Each synset contains a group of words that share a common meaning, and the relationships between synsets are captured in a network of nodes and edges.\nIn NLP, WordNet is often used for tasks such as word sense disambiguation and semantic similarity analysis. WordNet provides a way to group words with similar meanings, which can be helpful in determining the intended meaning of a word in context. For example, if a sentence contains the word “bank,” WordNet can be used to identify whether it refers to a financial institution or a river bank, based on the surrounding words and the context of the sentence.\nOverall, WordNet is a useful tool for NLP researchers and practitioners who need to work with language data and want to leverage the relationships between words and concepts to improve their analyses.\nClick here for more information\n\nOpen Mind Common Sense\nOpen Mind Common Sense (OMCS) is a knowledge base developed by the MIT Media Lab, which aims to provide a large, structured database of common-sense knowledge that can be used in natural language processing applications. OMCS is created through crowdsourcing and allows people to enter statements about the world in natural language, which are then parsed and stored in a database. This knowledge base can be used to help NLP systems better understand the meaning behind natural language and make more accurate predictions about human behavior and actions. The heuristic approach of OMCS involves manually designing a structured database of knowledge, which can be queried and used to improve NLP applications.\n\n\n\nAdvantages\nHeuristic methods in NLP have several advantages.\n\nFirst, they do not require large amounts of labeled data, which can be time-consuming and expensive to obtain.\nSecond, heuristic methods can be easily customized for specific applications and domains.\nThird, they are often more interpretable than machine learning or deep learning methods, allowing for better understanding of how the system is making its decisions.\nFourth, heuristic methods can be faster to implement and run than more complex approaches.\n\n\n\nCurrent Applications of Heuristic Methods in Natural Language Processing\nHeuristic methods have been used in Natural Language Processing (NLP) for decades, and they continue to be used in many current NLP applications. One such application is sentiment analysis, where heuristic rules are used to identify positive, negative, or neutral sentiment in text. Heuristic methods are also used in named entity recognition, where rules based on language patterns are used to identify and classify entities like names, organizations, and locations. Another application is question answering, where heuristic methods are used to generate answers based on patterns in the question and knowledge sources.\nIn addition to these applications, heuristic methods are also used for text classification, information extraction, and summarization. While deep learning models have recently gained popularity in NLP, heuristic methods are still widely used because they are often more interpretable and easier to modify than complex neural networks. Furthermore, heuristic methods can be useful in scenarios where large amounts of annotated data are not available, and rule-based methods can be created and fine-tuned with smaller datasets.\nOverall, heuristic methods continue to play an important role in NLP, and they are likely to remain a key component of NLP systems in the future.\n\n\nMachine learning-based methods\n\nWith the advent of machine learning, researchers began developing models that learned patterns and relationships in language data. These models could be trained on large datasets, allowing them to capture more complex language structures.\n\n\nAdvantages of Machine Learning Methods over Heuristic Methods in NLP\nMachine learning methods have become increasingly popular in Natural Language Processing (NLP) due to their numerous advantages over heuristic methods. Some of the advantages are:\n\nLearning from Data Machine learning algorithms can learn from large datasets and generalize well on unseen data, which is not possible with heuristic methods.\nAdaptability Machine learning methods can adapt to new data and improve their performance over time, whereas heuristic methods require manual tuning for each task.\nHigher Accuracy Machine learning methods can achieve higher accuracy compared to heuristic methods, especially in complex tasks such as speech recognition, sentiment analysis, and machine translation.\nTime-Saving Machine learning methods can automate tasks that would be time-consuming or impossible to perform manually, such as text classification or clustering.\nScalability Machine learning methods can scale to large datasets and handle high-dimensional data, which is a challenge for heuristic methods.\n\nMachine learning methods can scale to large datasets and handle high-dimensional data, which is a challenge for heuristic methods.\n\n\n\nMachine learning algorithms used in NLP\nThere are several machine learning algorithms used in NLP, some of which are Naive Bayes - a probabilistic algorithm that works well for text classification tasks such as spam filtering, sentiment analysis, and topic categorization.\nSupport Vector Machines (SVM) - a supervised learning algorithm that can be used for text classification and sequence labeling tasks.\nLogistic Regression - A type of algorithm used for binary classification problems, where the goal is to predict whether a given input belongs to one of two possible classes.\nLDA (Latent Dirichlet Allocation) - A probabilistic model used for topic modeling in NLP, where the goal is to identify topics within a collection of documents.\nHidden Markov Models - A statistical model used for sequence prediction problems, where the goal is to predict the sequence of states or outputs based on observed data. It is commonly used for speech recognition and named entity recognition.\n\n\n\nDeep learning-based methods\nDeep learning, a subset of machine learning, involves the use of artificial neural networks with multiple layers to learn increasingly complex patterns. Deep learning-based models, such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), have shown significant success in NLP tasks such as language modeling, machine translation, and sentiment analysis. These models require large amounts of data and computing power to train, but they have led to many breakthroughs in the field.\n\n\n\nAdvantages of Deep Learning Methods in NLP\nDeep learning methods have several advantages over traditional machine learning methods in NLP:\n\nBetter performance: Deep learning models can achieve higher accuracy levels than traditional machine learning models in complex NLP tasks such as language translation and sentiment analysis.\nFeature learning: Deep learning algorithms can automatically learn features from the data, eliminating the need for feature engineering, which is often required in traditional machine learning.\nHandling large datasets: Deep learning algorithms are better suited for handling large datasets, as they can learn from a vast amount of data without overfitting or losing accuracy.\nGeneralization: Deep learning models can generalize better to unseen data, allowing them to make accurate predictions on new data points.\n\n\n\n\nCommon Neural Network Architectures for NLP\nNeural networks have been very effective in natural language processing tasks. Here are some popular architectures used in NLP:\n\nRecurrent Neural Network (RNN): It is a type of neural network where connections between nodes form a directed cycle. RNNs are useful for processing sequential data.\nLong Short-Term Memory (LSTM): It is a type of RNN that solves the vanishing gradient problem and allows the model to remember important information from earlier in the sequence.\nGated Recurrent Unit (GRU)/Convolutional Neural Network (CNN): These are alternative RNN architectures that have been shown to work well for certain NLP tasks.\nTransformers: It is a newer architecture that has gained popularity in recent years, especially in tasks such as language modeling and machine translation. It is based on a self-attention mechanism that allows the model to focus on different parts of the input sequence.\nAutoencoders: It is a type of neural network that can be used for unsupervised learning tasks such as text generation or representation learning."
  },
  {
    "objectID": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#challenges-in-nlp",
    "href": "blogs/Natural_Language_Processing(NLP)/Natural_Language_Processing(NLP).html#challenges-in-nlp",
    "title": "Natural Language Processing(NLP)",
    "section": "Challenges in NLP",
    "text": "Challenges in NLP\n\nAmbiguity\nOne of the major challenges in Natural Language Processing (NLP) is the ambiguity of human language. Ambiguity refers to situations where a word or a sentence can have multiple meanings or interpretations. For example, the word “bank” can refer to a financial institution or a riverbank. Similarly, the sentence “I saw her duck” can mean “I saw the bird she owns” or “I saw her physically duck down.” This creates confusion for NLP models that rely on clear and unambiguous language. NLP researchers tackle the challenge of ambiguity by developing algorithms that can understand the context in which words are used and disambiguate the multiple meanings based on the context.\n\n\nDealing with Contextual Words\nContextual words are words that take on different meanings depending on the context in which they are used.This poses a major challenge in NLP as machines may not be able to accurately interpret the intended meaning of such words. For example, the word “bank” can refer to a financial institution or a river bank. Another example is the word “bass” which can refer to a fish or a low-frequency sound in music.\nTo address this challenge, NLP models are designed to consider the context of the word within the sentence, paragraph or document in order to determine the appropriate meaning. Some popular techniques used to deal with contextual words include word sense disambiguation and named entity recognition. However, the complexity of language and the vast number of possible contexts means that this remains an ongoing challenge in NLP.\n\n\nColloquialisms and slang\nColloquialisms and slang are informal words and expressions used in everyday language, which can pose a challenge in NLP. These words may not have a clear definition or meaning, and their usage can vary based on context and culture. For example, the slang term “lit” can mean exciting or under the influence of drugs, depending on the context. Handling colloquialisms and slang is particularly important in social media analysis and chatbot development, where informal language is commonly used.\n\n\nSynonyms\nOne of the major challenges in natural language processing (NLP) is dealing with synonyms, which are words that have similar meanings but are different in their spelling or usage. This creates ambiguity and difficulty in understanding the true meaning of a sentence.For example, “buy” and “purchase” are synonyms, but they may be used differently in different contexts, such as “I bought a new car” versus “I purchased a new house.” NLP algorithms must be able to accurately identify and interpret synonyms in order to understand the intended meaning of a sentence.\n\n\nIrony, Sarcasm, and Tonal Differences\nOne of the significant challenges in NLP is detecting the tone of a text, especially irony and sarcasm. This is because the tone is often conveyed through the context and cannot be determined solely based on the words used.For example, the statement “Oh great, another meeting” can be interpreted as positive or negative depending on the tone. Additionally, different regions or cultures may use different tonal expressions, making it difficult for NLP models to accurately detect them.\n\n\nSpelling Errors\nSpelling errors are a common challenge in NLP, as they can significantly affect the accuracy of text analysis.For example, the word “teh” instead of “the” can cause confusion for NLP algorithms. This challenge is particularly prevalent in user-generated content such as social media posts, where people often use abbreviations or non-standard spellings. NLP techniques such as spell checking and correction can help mitigate this issue, but they may not always be foolproof.\n\n\nCreativity\nExplanation: One of the significant challenges in NLP is the ability to generate creative and novel sentences. It involves generating language that is not just grammatically correct but also unique and meaningful. Current NLP models struggle with producing original content, especially in tasks like text generation, poetry, and storytelling. For instance, a model might be able to generate a coherent sentence, but it may not be imaginative or creative.\nExample: A simple example would be generating a poem. While current NLP models can produce a grammatically correct poem, they might not be able to create a poem that is creative, has rhyme or meter, and evokes emotions in the reader. This is because generating creative content involves understanding not just the meaning of words but also their associations, connotations, and cultural significance.\n\n\nDiversity\nOne of the significant challenges in NLP is dealing with diversity in language, which includes differences in language structures, styles, dialects, and cultures. For example, different regions may have different ways of expressing the same idea, making it difficult to develop universal language models. Additionally, languages themselves can have varying levels of complexity, making it challenging to develop models that can handle the nuances of each language. An example of this challenge is that many languages have multiple scripts, and some languages may not even have a written form."
  },
  {
    "objectID": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html",
    "href": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html",
    "title": "Day 1 Web Scraping and AI Summarization",
    "section": "",
    "text": "Import necessary libraries: openai, os, requests, BeautifulSoup, load_dotenv, display, and Markdown.\nThese libraries are used for interacting with the OpenAI API, handling operating system interactions, making HTTP requests, parsing HTML, loading environment variables, and displaying Markdown content.\nDependencies can be installed using requirements.txt.\nCode:\n# importing the libaries\nimport openai\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nfrom IPython.display import display, Markdown\n\n\n\n\n\nLoad environment variables from the .env file using load_dotenv().\nSet the OpenAI API key using openai.api_key = os.getenv(\"OPENAI_API_KEY\").\nCode:\n# load the environment variables from .env file \nload_dotenv()\n\n# set the OpenAI API key \nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n\n\n\nThe Website class is defined to fetch and parse website content.\nIt takes a URL as input, retrieves the HTML content using requests.get() with headers to mimic a browser, and parses it with BeautifulSoup.\nThe class extracts the title and visible text of the webpage, removing irrelevant elements like script, style, img, and input tags.\nCode:\n# Some websites need you to use proper headers when fetching them:\nheaders = {\n    # The User-Agent header is used to identify the client making the request.\n    # This helps in mimicking a real browser to avoid being blocked by websites.\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nclass Website:\n    def __init__(self, url):\n        \"\"\"\n        Initialize a Website object from the given URL using the BeautifulSoup library.\n\n        Args:\n            url (str): The URL of the website to fetch and parse.\n        \"\"\"\n        self.url = url  # Store the URL of the website.\n\n        # Send an HTTP GET request to the URL with the specified headers.\n        response = requests.get(url, headers=headers)\n\n        # Parse the HTML content of the response using BeautifulSoup.\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Extract the title of the webpage, if available. If not, set it to \"No title found\".\n        self.title = soup.title.string if soup.title else \"No title found\"\n\n        # Remove irrelevant elements like &lt;script&gt;, &lt;style&gt;, &lt;img&gt;, and &lt;input&gt; from the body of the webpage.\n        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n            irrelevant.decompose()\n\n        # Extract the visible text from the body of the webpage, separating lines with a newline character.\n        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n\n\n\n\n\nSystem Prompt: Instructs the AI on its role and desired output format (e.g., summarizing a website in markdown).\nUser Prompt: Contains the specific request or query for the AI, such as summarizing the content of a given website.\nCode:\n# Define our system prompt\nsystem_prompt = \"You are an assistant that analyzes the contents of a website \\\nand provides a short summary, ignoring text that might be navigation related. \\\nRespond in markdown.\"\n\n# A function that writes a User Prompt that asks for summaries of websites:\ndef user_prompt_for(website):\n    user_prompt = f\"You are looking at a website titled {website.title}\\n\"\n    user_prompt += \"The contents of this website is as follows; \\\n    please provide a short summary of this website in markdown. \\\n    If it includes news or announcements, then summarize these too.\\n\\n\"\n    user_prompt += website.text\n    return user_prompt\n\n\n\n\n\nThe OpenAI API requires messages to be structured as a list of dictionaries, with each dictionary containing a ‘role’ (system or user) and ‘content’ (the prompt).\nCode:\ndef message_for(website):\n    return [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': user_prompt_for(website)}\n    ]\n\n\n\n\n\nThe summarize() function takes a URL, creates a Website object, constructs the messages with the system and user prompts, and calls the OpenAI API to generate a summary.\nThe generated summary is then returned.\nCode:\ndef summarize(url):\n    website = Website(url)\n    response = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=message_for(website)\n    )\n    return response.choices[0].message.content\n\n\n\n\n\nThe display(Markdown()) function is used to display the website summaries in Markdown format.\nCode:\ndisplay(Markdown(summarize(\"[https://edwarddonner.com](https://edwarddonner.com)\")))\ndisplay(Markdown(summarize(\"[https://cnn.com](https://cnn.com)\")))\ndisplay(Markdown(summarize(\"[https://anthropic.com](https://anthropic.com)\")))\n\n\n\n\ndisplay(Markdown(summarize(\"[https://anthropic.com](https://anthropic.com)\")))"
  },
  {
    "objectID": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html#web-scraping-and-ai-summarization",
    "href": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html#web-scraping-and-ai-summarization",
    "title": "Day 1 Web Scraping and AI Summarization",
    "section": "",
    "text": "Import necessary libraries: openai, os, requests, BeautifulSoup, load_dotenv, display, and Markdown.\nThese libraries are used for interacting with the OpenAI API, handling operating system interactions, making HTTP requests, parsing HTML, loading environment variables, and displaying Markdown content.\nDependencies can be installed using requirements.txt.\nCode:\n# importing the libaries\nimport openai\nimport os\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nfrom IPython.display import display, Markdown\n\n\n\n\n\nLoad environment variables from the .env file using load_dotenv().\nSet the OpenAI API key using openai.api_key = os.getenv(\"OPENAI_API_KEY\").\nCode:\n# load the environment variables from .env file \nload_dotenv()\n\n# set the OpenAI API key \nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n\n\n\nThe Website class is defined to fetch and parse website content.\nIt takes a URL as input, retrieves the HTML content using requests.get() with headers to mimic a browser, and parses it with BeautifulSoup.\nThe class extracts the title and visible text of the webpage, removing irrelevant elements like script, style, img, and input tags.\nCode:\n# Some websites need you to use proper headers when fetching them:\nheaders = {\n    # The User-Agent header is used to identify the client making the request.\n    # This helps in mimicking a real browser to avoid being blocked by websites.\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n}\n\nclass Website:\n    def __init__(self, url):\n        \"\"\"\n        Initialize a Website object from the given URL using the BeautifulSoup library.\n\n        Args:\n            url (str): The URL of the website to fetch and parse.\n        \"\"\"\n        self.url = url  # Store the URL of the website.\n\n        # Send an HTTP GET request to the URL with the specified headers.\n        response = requests.get(url, headers=headers)\n\n        # Parse the HTML content of the response using BeautifulSoup.\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        # Extract the title of the webpage, if available. If not, set it to \"No title found\".\n        self.title = soup.title.string if soup.title else \"No title found\"\n\n        # Remove irrelevant elements like &lt;script&gt;, &lt;style&gt;, &lt;img&gt;, and &lt;input&gt; from the body of the webpage.\n        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n            irrelevant.decompose()\n\n        # Extract the visible text from the body of the webpage, separating lines with a newline character.\n        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n\n\n\n\n\nSystem Prompt: Instructs the AI on its role and desired output format (e.g., summarizing a website in markdown).\nUser Prompt: Contains the specific request or query for the AI, such as summarizing the content of a given website.\nCode:\n# Define our system prompt\nsystem_prompt = \"You are an assistant that analyzes the contents of a website \\\nand provides a short summary, ignoring text that might be navigation related. \\\nRespond in markdown.\"\n\n# A function that writes a User Prompt that asks for summaries of websites:\ndef user_prompt_for(website):\n    user_prompt = f\"You are looking at a website titled {website.title}\\n\"\n    user_prompt += \"The contents of this website is as follows; \\\n    please provide a short summary of this website in markdown. \\\n    If it includes news or announcements, then summarize these too.\\n\\n\"\n    user_prompt += website.text\n    return user_prompt\n\n\n\n\n\nThe OpenAI API requires messages to be structured as a list of dictionaries, with each dictionary containing a ‘role’ (system or user) and ‘content’ (the prompt).\nCode:\ndef message_for(website):\n    return [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': user_prompt_for(website)}\n    ]\n\n\n\n\n\nThe summarize() function takes a URL, creates a Website object, constructs the messages with the system and user prompts, and calls the OpenAI API to generate a summary.\nThe generated summary is then returned.\nCode:\ndef summarize(url):\n    website = Website(url)\n    response = openai.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=message_for(website)\n    )\n    return response.choices[0].message.content\n\n\n\n\n\nThe display(Markdown()) function is used to display the website summaries in Markdown format.\nCode:\ndisplay(Markdown(summarize(\"[https://edwarddonner.com](https://edwarddonner.com)\")))\ndisplay(Markdown(summarize(\"[https://cnn.com](https://cnn.com)\")))\ndisplay(Markdown(summarize(\"[https://anthropic.com](https://anthropic.com)\")))\n\n\n\n\ndisplay(Markdown(summarize(\"[https://anthropic.com](https://anthropic.com)\")))"
  },
  {
    "objectID": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html#recent-announcements",
    "href": "blogs/Day 1  Web Scraping and AI Summarization/Day 1  Web Scraping and AI Summarization.html#recent-announcements",
    "title": "Day 1 Web Scraping and AI Summarization",
    "section": "Recent Announcements",
    "text": "Recent Announcements\n\nISO 42001 Certification: An announcement regarding achieving certification in responsible AI practices.\nClaude 3.7 Sonnet Launch: Announcement of the release of the latest iteration of Claude, touted as the most intelligent AI model yet.\n\nAnthropic positions itself as a leader in developing AI with a focus on safety and long-term societal benefits."
  }
]