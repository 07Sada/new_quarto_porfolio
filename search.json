[
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Blogs",
    "section": "",
    "text": "Supervised Learning Algorithms\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Docker\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Selection Techniques in Machine Learning\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Algorithm\n\n\n\n\n\n\n\n\n\n\n\nSep 26, 2024\n\n\nSadashiv\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html",
    "href": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html",
    "title": "Introduction to Algorithm",
    "section": "",
    "text": "In the context of machine learning and deep learning, an algorithm is like a set of step-by-step instructions that a computer follows to learn from data or make predictions.\nIt’s a recipe for the computer to process information, find patterns, and make decisions.\nThink of it as a smart way for a computer to solve problems or make sense of complex data.\nThese algorithms help us build models that can recognize patterns, understand language, or even play games, among many other things.\n\n\n\n\nIn machine learning and deep learning, there are several types of algorithms, each with its own purpose and characteristics. Here’s a simple overview of some common types:\n\n[[Supervised Learning Algorithms]]:\n\nThese algorithms learn from labeled data, where the input and the desired output are known.\nCommon algorithms include Linear Regression, Decision Trees, and Support Vector Machines (SVM). \n\n[[Unsupervised Learning Algorithms]]:\n\nThese algorithms work with unlabeled data, aiming to find hidden patterns or group similar data points.\nExamples include K-Means Clustering and Principal Component Analysis (PCA).\n\nDeep Learning Algorithms:\n\nDeep learning is a subset of machine learning that uses neural networks with many layers (deep networks) to learn from data.\nCommon deep learning algorithms include Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequence data.\n\nReinforcement Learning Algorithms:\n\nThese algorithms are used in scenarios where an agent learns to take actions to maximize rewards in an environment.\nWell-known algorithms include Q-Learning and Deep Q-Networks (DQNs).\n\nSemi-Supervised and Self-Supervised Learning:\n\nThese approaches combine elements of supervised and unsupervised learning, making use of both labeled and unlabeled data.\n\nTransfer Learning:\n\nTransfer learning involves using a pre-trained model on a related task as a starting point for a new task, saving training time and resources.\n\nEnsemble Methods:\n\nEnsemble methods combine multiple models to improve prediction accuracy. Examples include Random Forests and Gradient Boosting.\n\nNatural Language Processing (NLP) Algorithms:\n\nThese are specialized algorithms for working with text data, such as sentiment analysis and named entity recognition.\n\nDeep Reinforcement Learning:\n\nThis combines deep learning and reinforcement learning, often used in complex tasks like game playing (e.g., AlphaGo).\n\nDimensionality Reduction:\n\nAlgorithms like t-SNE and UMAP are used to reduce the number of features in high-dimensional data while preserving important relationships.\n\n\nThese are just some of the many types of algorithms used in machine learning and deep learning. The choice of algorithm depends on the specific problem you’re trying to solve and the type of data you have."
  },
  {
    "objectID": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html#introduction-to-algorithm",
    "href": "blogs/Introduction_to_Algorithm/Introduction to Algorithm.html#introduction-to-algorithm",
    "title": "Introduction to Algorithm",
    "section": "",
    "text": "In the context of machine learning and deep learning, an algorithm is like a set of step-by-step instructions that a computer follows to learn from data or make predictions.\nIt’s a recipe for the computer to process information, find patterns, and make decisions.\nThink of it as a smart way for a computer to solve problems or make sense of complex data.\nThese algorithms help us build models that can recognize patterns, understand language, or even play games, among many other things.\n\n\n\n\nIn machine learning and deep learning, there are several types of algorithms, each with its own purpose and characteristics. Here’s a simple overview of some common types:\n\n[[Supervised Learning Algorithms]]:\n\nThese algorithms learn from labeled data, where the input and the desired output are known.\nCommon algorithms include Linear Regression, Decision Trees, and Support Vector Machines (SVM). \n\n[[Unsupervised Learning Algorithms]]:\n\nThese algorithms work with unlabeled data, aiming to find hidden patterns or group similar data points.\nExamples include K-Means Clustering and Principal Component Analysis (PCA).\n\nDeep Learning Algorithms:\n\nDeep learning is a subset of machine learning that uses neural networks with many layers (deep networks) to learn from data.\nCommon deep learning algorithms include Convolutional Neural Networks (CNNs) for image analysis and Recurrent Neural Networks (RNNs) for sequence data.\n\nReinforcement Learning Algorithms:\n\nThese algorithms are used in scenarios where an agent learns to take actions to maximize rewards in an environment.\nWell-known algorithms include Q-Learning and Deep Q-Networks (DQNs).\n\nSemi-Supervised and Self-Supervised Learning:\n\nThese approaches combine elements of supervised and unsupervised learning, making use of both labeled and unlabeled data.\n\nTransfer Learning:\n\nTransfer learning involves using a pre-trained model on a related task as a starting point for a new task, saving training time and resources.\n\nEnsemble Methods:\n\nEnsemble methods combine multiple models to improve prediction accuracy. Examples include Random Forests and Gradient Boosting.\n\nNatural Language Processing (NLP) Algorithms:\n\nThese are specialized algorithms for working with text data, such as sentiment analysis and named entity recognition.\n\nDeep Reinforcement Learning:\n\nThis combines deep learning and reinforcement learning, often used in complex tasks like game playing (e.g., AlphaGo).\n\nDimensionality Reduction:\n\nAlgorithms like t-SNE and UMAP are used to reduce the number of features in high-dimensional data while preserving important relationships.\n\n\nThese are just some of the many types of algorithms used in machine learning and deep learning. The choice of algorithm depends on the specific problem you’re trying to solve and the type of data you have."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html",
    "title": "Introduction to Docker",
    "section": "",
    "text": "Docker is a platform designed to help developers build, share, and run container applications."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#what-is-docker",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#what-is-docker",
    "title": "Introduction to Docker",
    "section": "",
    "text": "Docker is a platform designed to help developers build, share, and run container applications."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#why-do-we-need-dockers",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#why-do-we-need-dockers",
    "title": "Introduction to Docker",
    "section": "Why do we need Dockers?",
    "text": "Why do we need Dockers?\n\nConsistency Across Environments\n\n\nProblem: Applications often behave differently in development, testing, and production environments due to variations in configurations, dependencies, and infrastructure.\nSolution: Docker containers encapsulate all the necessary components, ensuring the application runs consistently across all environments.\n\n\n\nIsolation\n\n\nProblem: Running multiple applications on the same host can lead to conflicts, such as dependency clashes or resource contention.\nSolution: Docker provides isolated environments for each application, preventing interference and ensuring stable performance.\n\n\n\nScalability\n\n\nProblem: Scaling applications to handle increased load can be challenging, requiring manual intervention and configuration.\nSolution: Docker makes it easy to scale applications horizontally by running multiple container instances, allowing for quick and efficient scaling."
  },
  {
    "objectID": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#how-exactly-docker-is-used",
    "href": "blogs/Introduction_to_Docker/Introduction_to_Docker.html#how-exactly-docker-is-used",
    "title": "Introduction to Docker",
    "section": "How exactly Docker is used?",
    "text": "How exactly Docker is used?\n\n\n\nImage credit: Geeksforgeeks\n\n\n\nDocker Engine\nDocker Engine is the core component of the Docker platform, responsible for creating, running, and managing Docker containers. It serves as the runtime that powers Docker’s containerization capabilities. Here’s an in-depth look at the Docker Engine:\n\n\nComponents of Docker Engine\n\n1. Docker Daemon (dockerd):\n\nFunction : The Docker daemon is the background service running on the host machine. It manages Docker objects such as images, containers, networks, and volumes.\nInteraction : It listens for Docker API requests and processes them, handling container lifecycle operations (start, stop, restart, etc.).\n\n\n\n2. Docker CLI (docker):\n\nFunction : The Docker Command Line Interface (CLI) is the tool that users interact with to communicate with the Docker daemon.\nUsage : Users run Docker commands through the CLI to perform tasks like building images, running containers, and managing Docker resources.\n\n\n\n3. REST API :\n\nFunction : The Docker REST API allows communication between the Docker CLI and the Docker daemon. It also enables programmatic interaction with Docker.\nUsage : Developers can use the API to automate Docker operations or integrate Docker functionality into their applications.\n\n\n\n\nDocker Image\nA Docker image is a lightweight, stand-alone, and executable software package that includes everything needed to run a piece of software, such as the code, runtime, libraries, environment variables, and configuration files. Images are used to create Docker containers, which are instances of these images.\n\nComponents of a Docker Image\n\nBase Image : The starting point for building an image. It could be a minimal OS image like alpine, a full-fledged OS like ubuntu, or even another application image like python or node.\nApplication Code : The actual code and files necessary for the application to run.\nDependencies : Libraries, frameworks, and packages required by the application.\nMetadata : Information about the image, such as environment variables, labels, and exposed ports.\n\n\n\nDocker Image Lifecycle\n\nCreation : Images are created using the docker build command, which processes the instructions in a Dockerfile to create the image layers.\n\nStorage : Images are stored locally on the host machine. They can also be pushed to and pulled from Docker registries like Docker Hub, AWS ECR, or Google Container Registry.\n\nDistribution : Images can be shared by pushing them to a Docker registry, allowing others to pull and use the same image.\n\nExecution : Images are executed by running containers, which are instances of these images.\n\n\n\nDockerfile\nA Dockerfile is a text file that contains a series of instructions used to build a Docker image. Each instruction in a Dockerfile creates a layer in the image, allowing for efficient image creation and reuse of layers. Dockerfiles are used to automate the image creation process, ensuring consistency and reproducibility.\n\nKey Components of a Dockerfile\n\nBase Image (FROM) : Specifies the starting point for the image, which could be a minimal operating system, a specific version of a language runtime, or another image.\nExample: FROM ubuntu:20.04\nLabels (LABEL) : Adds metadata to the image, such as version, description, or maintainer.\nExample: LABEL version=\"1.0\" description=\"My application\"\nRun Commands (RUN) : Executes commands in the image during the build process, typically used to install software packages. Example: docker RUN apt-get update && apt-get install -y python3\nCopy Files (COPY): Copies files or directories from the host system to the image.\nExample: COPY . /app\nEnvironment Variables (ENV) : Sets environment variables in the image.\nExample: ENV PATH /app/bin:$PATH\nWork Directory (WORKDIR) : Sets the working directory for subsequent instructions.\nExample: WORKDIR /app\nExpose Ports (EXPOSE) : Informs Docker that the container listens on specified network ports.\nExample: EXPOSE 8080\nCommand (CMD) : Provides a default command to run when the container starts.\nExample: CMD [\"python\", \"app.py\"]\nVolume (VOLUME) : Creates a mount point with a specified path and marks it as holding externally mounted volumes from the host or other containers.\nExample: VOLUME [\"/data\"]\nArguments (ARG) : Defines build-time variables.\nExample: ARG VERSION=1.0\n\n# Use an Official python runtime as a base image\nFROM python:3.8-slim\n\n# Set the working directory in the container\nWORKDIR /app \n\n# Copy the current directory contents into the container at /app \nCOPY . /app \n\n# Install the needed packages specified in requirements.txt\nRUN pip install -no-cache-dir -r requirements.txt \n\n# Make port 80 available to the world outside this container \nEXPOSE 80\n\n# Define enviroment vaiable\nENV NAME world\n\n# Run app.py when the container launches \nCMD [\"python\", \"app.py\"]\n\n\n\nDocker Container\nA Docker container is a lightweight, portable, and isolated environment that encapsulates an application and its dependencies, allowing it to run consistently across different computing environments. Containers are created from Docker images, which are immutable and contain all the necessary components for the application to run.\n\n\n\n\nRegistry\nA Docker registry is a service that stores and distributes Docker images. It acts as a repository where users can push, pull, and manage Docker images. Docker Hub is the most well-known public registry, but private registries can also be set up to securely store and manage images within an organization.\n\nKey Components of a Docker Registry:\n\nRepositories : A repository is a collection of related Docker images, typically different versions of the same application. Each repository can hold multiple tags, representing different versions of an image.\nTags : Tags are used to version images within a repository.\nFor example, myapp:1.0, myapp:2.0, and myapp:latest are tags for different versions of the myapp image.\n\n\n\nTypes of Docker Registries\n\nDocker Hub:\n\n\nDescription : The default public registry provided by Docker, which hosts a vast number of public images and also supports private repositories.\n\nURL : hub.docker.com\n\nUse Case : Publicly sharing images and accessing a large collection of pre-built images from the community and official repositories.\n\nPrivate Registries :\n\n\nDescription : Custom registries set up by organizations to securely store and manage their own Docker images.\n\nUse Case : Ensuring security and control over image distribution within an organization.\n\nThird-Party Registries :\n\n\nExamples : Amazon Elastic Container Registry (ECR), Google Container Registry (GCR), Azure Container Registry (ACR).\n\nUse Case : Integrating with cloud platforms for seamless deployment and management of images within cloud infrastructure.\n\n\n\n\nBenefits of Using Docker Registries\n1. Centralized Image Management\n\nRegistries provide a centralized location to store and manage Docker images, making it easier to organize and distribute them.\n2. Version Control :\n\nUsing tags, registries allow version control of images, enabling users to easily roll back to previous versions if needed.\n3. Collaboration:\n\nPublic registries like Docker Hub facilitate collaboration by allowing users to share images with the community or within teams.\n4. Security :\n\nPrivate registries ensure that sensitive images are stored securely and access is controlled within an organization.\n5. Integration with CI/CD :\n\nRegistries integrate seamlessly with CI/CD pipelines, automating the process of building, storing, and deploying Docker images.\n\n\n\nUse-cases\n\n1. Microservices Architecture\n\n\nDescription : Microservices break down applications into smaller, independent services, each running in its own container.\nBenefits : Simplifies deployment, scaling, and maintenance. Each service can be developed, updated, and deployed independently.\n\n\n\n2. Continuous Integration and Continuous Deployment (CI/CD)\n\n\nDescription : Docker ensures a consistent environment from development through testing to production.\nBenefits : Streamlines the CI/CD pipeline, reduces discrepancies between environments, and speeds up testing and deployment processes.\n\n\n\n3. Cloud Migration\n\n\nDescription : Containerizing applications to move them to the cloud.\nBenefits : Simplifies the migration process, allows applications to run consistently across different cloud providers, and optimizes resource usage.\n\n\n\n4. Scalable Web Applications\n\n\nDescription : Deploying web applications in containers for easy scaling.\nBenefits : Simplifies scaling up or down based on traffic, ensures consistent deployment, and enhances resource utilization.\n\n\n\n5. Testing and QA\n\n\nDescription : Creating consistent environments for testing applications.\nBenefits : Ensures tests are run in environments identical to production, speeds up the setup of test environments, and facilitates automated testing.\n\n\n\n6. Machine Learning and AI\n\n\nDescription : Deploying machine learning models and AI applications in containers.\nBenefits : Ensures consistency in the runtime environment, simplifies scaling of model training and inference, and facilitates collaboration and reproducibility.\n\n\n\n7. API Development and Deployment\n\n\nDescription : Developing and deploying APIs in containers.\nBenefits : Ensures APIs run consistently across environments, simplifies scaling, and improves deployment speed and reliability."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Vegetable Recognition\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nSadashiv\n\n\n\n\n\n\n\n\n\n\n\n\nBrand Detection\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2024\n\n\nSadashiv\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/Brand Detection/Brand_Detection.html",
    "href": "projects/Brand Detection/Brand_Detection.html",
    "title": "Brand Detection",
    "section": "",
    "text": "Brand Detection\nVisual content, such as videos and images, plays a significant role in modern-day marketing. Traditionally, brands have had to pay content creators to feature their brand logo in their content. However, marketers can now leverage ML-powered computer vision to identify and recognize their products in various forms of content, including videos and images. This technology enables marketers to extract valuable insights from the content and understand the audience’s behaviour better. With this understanding, brands can improve their advertising strategies and achieve higher ROI by targeting their audience more effectively and personalizing their messaging. The potential benefits of ML-powered computer vision in marketing make it an exciting area of exploration for brands and marketers.\n\n\n\nimage\n\n\n\n\nInstruction to train the model in Google Colab\n!rm -r /content/sample_data; # remove the sample directory from google colab\n!git clone https://github.com/07Sada/brand.git # clone the repository\n# change the directory\n%cd /content/brand \n# install the requirements\n%pip install -r /content/brand/requirements.txt -q\n# initiate the training\nfrom BrandRecognition.pipeline.training_pipeline import TrainPipeline\nobj = TrainPipeline()\nobj.run_pipeline()\n\n\nScreenshots\n\n\n\nimage\n\n\n\n\nDemo\n\n\n\nPoject_video_#01\n\n\n\n\n\nPoject_video_#02"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html",
    "title": "Vegetable Recognition",
    "section": "",
    "text": "This notebook demonstrates a deep learning approach to classifying vegetable types from images, a task with applications in agriculture and retail. The model is built using PyTorch and features a custom ResNet9 architecture tailored for effective image classification. The notebook outlines the key steps in data preprocessing, model architecture, training, and evaluation, providing a comprehensive walkthrough of the process."
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#downloading-the-dataset",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#downloading-the-dataset",
    "title": "Vegetable Recognition",
    "section": "Downloading the dataset",
    "text": "Downloading the dataset\n\n\nCode\n# installing opendatasets to download the dataset from kaggle\n%pip install opendatasets -q\n\n\n\n\nCode\n# installing albumentations library for image transformation\n%pip install albumentations -q\n\n\n\n\nCode\n# importing opendatasets\nimport opendatasets as od\n\n# dataset url path\nurl = \"https://www.kaggle.com/datasets/misrakahmed/vegetable-image-dataset?datasetId=1817999&sortBy=voteCount\"\n\n# download the dataset\nod.download(url)\n\n\nDownloading vegetable-image-dataset.zip to ./vegetable-image-dataset\n\n\n100%|██████████| 534M/534M [00:04&lt;00:00, 117MB/s]\n\n\n\n\n\n\n\nCode\n# imorting dependancies\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport albumentations as A\nimport torch\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport torch.nn as nn\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nimport os\n\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#introduction-to-dataset",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#introduction-to-dataset",
    "title": "Vegetable Recognition",
    "section": "Introduction to Dataset",
    "text": "Introduction to Dataset\n\n\nCode\n# dataset path\nDATASET_PATH = \"./vegetable-image-dataset/Vegetable Images\"\nTRAIN_DATASET = DATASET_PATH + '/train'\nVALIDATION_DATASET = DATASET_PATH + '/validation'\n\nprint(f\"Total Categories in the dataset: {len(os.listdir(TRAIN_DATASET))}\")\n\nCATEGORIES_COUNT = {}\nCATEGORIES_LIST = os.listdir(TRAIN_DATASET)\n\nfor i in CATEGORIES_LIST:\n  CATEGORIES_COUNT[i] = len(os.listdir(str(TRAIN_DATASET) + \"/\" +str(i)))\n\nprint(CATEGORIES_COUNT)\nprint('\\n')\n\nCATEGORIES_COUNT = list(CATEGORIES_COUNT.values())\nplt.pie(CATEGORIES_COUNT, labels=CATEGORIES_LIST, autopct='%1.1f%%');\n\n\nTotal Categories in the dataset: 15\n{'Cucumber': 1000, 'Capsicum': 1000, 'Papaya': 1000, 'Tomato': 1000, 'Cabbage': 1000, 'Pumpkin': 1000, 'Bitter_Gourd': 1000, 'Radish': 1000, 'Broccoli': 1000, 'Cauliflower': 1000, 'Bean': 1000, 'Carrot': 1000, 'Bottle_Gourd': 1000, 'Potato': 1000, 'Brinjal': 1000}"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#importing-the-dataset-into-pytorch",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#importing-the-dataset-into-pytorch",
    "title": "Vegetable Recognition",
    "section": "Importing the dataset into pytorch",
    "text": "Importing the dataset into pytorch\n\n\nCode\n# importing pytorch\nimport torch\n\n# checking the version of the torch\nprint(f\"torch version: {torch.__version__}\")\n\n# Importing Imagefolder\nfrom torchvision.datasets import ImageFolder\n\n\ntorch version: 2.0.1+cu118\n\n\n\n\nCode\ntrain_dataset = ImageFolder(root=TRAIN_DATASET)\n\n# total images in the train_dataset\nprint(f\"Total images in train dataset: {len(train_dataset)}\\n\")\n\n# checking sample image from the train dataset\nimg, label = train_dataset[10]\n\nprint(f\"Sample image from train dataset\")\nplt.imshow(img)\nplt.axis('OFF');\n\n\nTotal images in train dataset: 15000\n\nSample image from train dataset\n\n\n\n\n\n\n\n\n\n\n\nCode\nvalidation_dataset = ImageFolder(root=VALIDATION_DATASET)\n\n# total images in the validation_dataset\nprint(f\"Total images in train dataset: {len(validation_dataset)}\\n\")\n\n# checking sample image from the train dataset\nimg, label = validation_dataset[2000]\n\nprint(f\"Sample image from validation_dataset\")\nplt.imshow(img)\nplt.axis('OFF');\n\n\nTotal images in train dataset: 3000\n\nSample image from validation_dataset\n\n\n\n\n\n\n\n\n\nImage transformation\n\n\nCode\n# Creating a class for image transformation with albumentations library\nclass ImageFolder(Dataset):\n    def __init__(self, root_dir, transform=None):\n        super(ImageFolder, self).__init__()\n        self.data = []\n        self.root_dir = root_dir\n        self.transform = transform\n        self.class_names = os.listdir(root_dir)\n\n        for index, name in enumerate(self.class_names):\n            files = os.listdir(os.path.join(root_dir, name))\n            self.data += list(zip(files, [index] * len(files)))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        img_file, label = self.data[index]\n        root_and_dir = os.path.join(self.root_dir, self.class_names[label])\n        image = np.array(Image.open(os.path.join(root_and_dir, img_file)))\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image)\n            image = augmentations[\"image\"]\n\n        return image, label\n\n\ntransform = A.Compose(\n    [\n        A.Resize(width=64, height=64),\n        A.Rotate(limit=40, p=0.9, border_mode=cv2.BORDER_CONSTANT),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.9),\n        A.OneOf(\n            [\n                A.Blur(blur_limit=3, p=0.5),\n                A.ColorJitter(p=0.5),\n            ],\n            p=1.0,\n        ),\n        A.Normalize(\n            mean=[0, 0, 0],\n            std=[1, 1, 1],\n            max_pixel_value=255,\n        ),\n        ToTensorV2(),\n    ]\n)\n\n\n\n\nCode\n# Applying transformation on training_dataset and validation_dataset\ntrain_dataset = ImageFolder(root_dir=TRAIN_DATASET, transform=transform)\nvalidation_dataset = ImageFolder(root_dir=VALIDATION_DATASET, transform=transform)\n\n\n\n\nCode\n# checking a random image from train_dataset\nimg, label = train_dataset[100]\n\nprint(f\"Shape of the image: {img.shape}\") # checking the size of the image\n\nprint(\"\\nThe image after transformation\")\nprint(train_dataset[100]) # the pixel values are noramalized\nprint(\"\\n\")\n\nprint(f\"Sample image\")\nplt.imshow(img.permute((1,2,0))); #This module returns a view of the tensor input with its dimensions permuted.\n\n\nShape of the image: torch.Size([3, 64, 64])\n\nThe image after transformation\n(tensor([[[0.7098, 0.6980, 0.7216,  ..., 0.6157, 0.6000, 0.6118],\n         [0.7137, 0.7020, 0.7255,  ..., 0.6157, 0.5922, 0.5961],\n         [0.7216, 0.7098, 0.7255,  ..., 0.6078, 0.5686, 0.5686],\n         ...,\n         [0.7765, 0.7804, 0.8000,  ..., 0.7490, 0.7922, 0.7882],\n         [0.7686, 0.7882, 0.8235,  ..., 0.7843, 0.7882, 0.7843],\n         [0.7569, 0.7843, 0.8275,  ..., 0.8078, 0.7843, 0.7843]],\n\n        [[0.6431, 0.6392, 0.6706,  ..., 0.4431, 0.4353, 0.4353],\n         [0.6510, 0.6431, 0.6745,  ..., 0.4392, 0.4314, 0.4314],\n         [0.6627, 0.6510, 0.6706,  ..., 0.4314, 0.4196, 0.4196],\n         ...,\n         [0.6078, 0.6039, 0.6039,  ..., 0.6627, 0.7098, 0.7059],\n         [0.5804, 0.5804, 0.5725,  ..., 0.6980, 0.7059, 0.6980],\n         [0.5725, 0.5686, 0.5529,  ..., 0.7255, 0.6980, 0.7020]],\n\n        [[0.8235, 0.8118, 0.8431,  ..., 0.4627, 0.4667, 0.4627],\n         [0.8275, 0.8196, 0.8471,  ..., 0.4588, 0.4627, 0.4588],\n         [0.8392, 0.8275, 0.8431,  ..., 0.4471, 0.4549, 0.4549],\n         ...,\n         [0.6353, 0.6314, 0.6275,  ..., 0.7647, 0.8039, 0.7961],\n         [0.6078, 0.6078, 0.5922,  ..., 0.8000, 0.7961, 0.7804],\n         [0.5961, 0.5922, 0.5725,  ..., 0.8235, 0.7882, 0.7765]]]), 0)\n\n\nSample image"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#dataloader",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#dataloader",
    "title": "Vegetable Recognition",
    "section": "DataLoader",
    "text": "DataLoader\n\n\nCode\n# importing DataLoader\nfrom torch.utils.data import DataLoader\n\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\nprint(f\"cpu_count in machine: {NUM_WORKERS}\")\n\n# dataloader for training dataset\ntrain_dataloader = DataLoader(dataset = train_dataset,\n                              batch_size=BATCH_SIZE,\n                              num_workers=NUM_WORKERS,\n                              shuffle=True)\n\n# dataloader for validation dataset\nvalidation_dataloader = DataLoader(dataset=validation_dataset,\n                                   batch_size=BATCH_SIZE,\n                                   num_workers=NUM_WORKERS,\n                                   shuffle=False)\n\n\ncpu_count in machine: 2\n\n\n\n\nCode\n# Get image and label from train_dataloader\ntrain_dataloder_img, train_dataloder_label = next(iter(train_dataloader))\n\n# Print out the shapes\ntrain_dataloder_img.shape, train_dataloder_label.shape\n\n\n(torch.Size([32, 3, 64, 64]), torch.Size([32]))"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#visualize-some-images-from-dataloader",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#visualize-some-images-from-dataloader",
    "title": "Vegetable Recognition",
    "section": "Visualize some images from dataloader",
    "text": "Visualize some images from dataloader\n\n\nCode\nfrom torchvision.utils import make_grid # Make a grid of images.\n\ndef show_batch(data_loader):\n  for images, labels in data_loader:\n    fig, ax = plt.subplots(figsize = (15,8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n    break\n\nshow_batch(train_dataloader)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# device setup\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n\n\n'cuda'"
  },
  {
    "objectID": "projects/computer_vison_project/Vegetable_Recognition.html#model-training",
    "href": "projects/computer_vison_project/Vegetable_Recognition.html#model-training",
    "title": "Vegetable Recognition",
    "section": "Model Training",
    "text": "Model Training\n\n\nCode\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Create class for model training and evaluation\nclass ImageClassificationBase(nn.Module): ## --&gt; nn.Module--&gt; Base class for all neural network modules. --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n  def training_step(self, batch):\n    images, labels = batch\n    images, labels = images.to(device), labels.to(device)\n    out = self(images) # Generate predictions\n    loss = F.cross_entropy(input= out, # Predicted unnormalized logits\n                           target = labels) # Ground truth class indices or class probabilities\n    return loss\n\n  def validation_step(self, batch):\n    images, labels = batch\n    images, labels = images.to(device), labels.to(device)\n    out = self(images) # Generate predictions\n    loss = F.cross_entropy(input = out, # Predicted unnormalized logits\n                           target = labels) # Ground truth class indices or class probabilities\n    acc = accuracy(outputs = out, # Calculate the accuracy\n                   labels = labels)\n    return {'validation_loss': loss.detach(), 'validation_accuracy':acc}\n\n  def validation_epoch_end(self, outputs):\n    batch_losses = [x['validation_loss'] for x in outputs]\n    # combine the losses\n    epoch_loss = torch.stack(batch_losses).mean() # PyTorch torch.stack() method joins (concatenates) a sequence of tensors (two or more tensors) along a new dimension.\n    batch_accuracy = [x['validation_accuracy'] for x in outputs]\n    epoch_accuracy = torch.stack(batch_accuracy).mean()\n    return {'validation_loss':epoch_loss.item(), 'validation_accuracy':epoch_accuracy.item()}\n\n  # printing the results\n  def epoch_end(self, epoch, result):\n    print(f\"Epoch {epoch},\\n train_loss:{result['train_loss']}, \\n validation_loss: {result['validation_loss']}, \\n validation_accuracy: {result['validation_accuracy']}\")\n\ndef accuracy(outputs, labels):\n  _, preds = torch.max(outputs, dim=1)\n  return torch.tensor(torch.sum(preds == labels).item()/ len(preds))\n\n\n\n\nCode\n@torch.no_grad() # Context-manager that disabled gradient calculation.\n\ndef evaluate(model, validation_dataloader):\n  \"\"\" Evaluate the model's performance on the validation dataset\"\"\"\n  model.eval()\n  outputs = [model.validation_step(batch) for batch in validation_dataloader]\n  return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n  history = []\n  optimizer = opt_func(model.parameters(), lr)\n  for epoch in range(epochs):\n    # training\n    model.train()\n    train_loss = []\n    for batch in train_loader:\n      loss = model.training_step(batch) # training the model for each batch\n      train_loss.append(loss) # collecting the loss\n      loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.--&gt; https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch-tensor-backward\n      optimizer.step() # Performs a single optimization step (parameter update). --&gt; https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html\n      optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero. --&gt; https://stackoverflow.com/a/48009142\n                            # official_doc --&gt; https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html\n\n    # validation\n    result = evaluate(model, validation_dataloader)\n    result['train_loss'] = torch.stack(train_loss).mean().item()\n    history.append(result)\n  return history\n\n\n\n\nCode\n##Model Building\ndef conv_block(in_channels, out_channels, pool=False):\n\n  layers = [nn.Conv2d(in_channels=in_channels,    # Number of channels in the input image\n                      out_channels=out_channels,  # Number of channels produced by the convolution\n                      kernel_size=3, # Size of the convolving kernel\n                      padding=1), # Padding added to all four sides of the input\n            nn.BatchNorm2d(num_features=out_channels), # num_features (int)--&gt;'C'from an expected input of size (N,C,H,W) --&gt; https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html#torch.nn.BatchNorm2d\n            nn.ReLU(inplace =True)] #-&gt; https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#relu\n  if pool: layers.append(nn.MaxPool2d(kernel_size=2)) # --&gt; https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d\n  return nn.Sequential(*layers)\n\n\n\n\nCode\nclass ResNet9(ImageClassificationBase):\n  def __init__(self, in_channels, num_classes):\n    super().__init__()\n    # input: 32 x 3 x 64 x 64\n    self.conv1 = conv_block(in_channels, 64) # 32 x 64 x 64 x 64\n    self.conv2 = conv_block(64, 128, pool=True) # 32 x 128 x 32 x 32\n    self.res1 = nn.Sequential(conv_block(128, 128), # 32 x 128 x 32 x 32\n                              conv_block(128, 128)) # 32 x 128 x 32 x 32\n\n    self.conv3 = conv_block(128, 256, pool=True) # 32 X 256 x 16 x 16\n    self.conv4 = conv_block(256, 512, pool=True) # 32 x 256 x 8 x 8\n    self.res2 = nn.Sequential(conv_block(512, 512), # 32 x 512 x 8 x 8 --&gt; residual_blocks --&gt; ## https://towardsdatascience.com/resnets-residual-blocks-deep-residual-learning-a231a0ee73d2#:~:text=A%20residual%20block%20is%20a,layer%20in%20the%20main%20path.\n                              conv_block(512, 512)) # 32 x 512 x 8 x 8\n\n    self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d(1), # 32 x 512 x 1 x 1 --&gt; official_documenataion --&gt;https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveMaxPool2d.html#torch.nn.AdaptiveMaxPool2d --&gt; simplified_version--&gt; https://stackoverflow.com/a/55869581\n                                    nn.Flatten(), # Flattens a contiguous range of dims into a tensor --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html#torch.nn.Flatten --&gt;simplified_version --&gt; https://www.tutorialspoint.com/how-to-flatten-an-input-tensor-by-reshaping-it-in-pytorch\n                                    nn.Dropout(0.2), # During training, randomly zeroes some of the elements of the input tensor --&gt; https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout --&gt; https://www.geeksforgeeks.org/dropout-in-neural-networks/\n                                    nn.Linear(in_features = 512,\n                                              out_features = num_classes)) # Applies a linear transformation to the incoming data --&gt;https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear --&gt;https://stackoverflow.com/a/54924812\n\n  def forward(self, xb):\n    out = self.conv1(xb)\n    out = self.conv2(out)\n    out = self.res1(out) + out\n    out = self.conv3(out)\n    out = self.conv4(out)\n    out = self.res2(out) + out\n    out = self.classifier(out)\n    return out\n\n\n\n\nCode\nmodel = ResNet9(3, len(train_dataset.class_names)).to(device)\nmodel\n\n\nResNet9(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res1): Sequential(\n    (0): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (conv3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res2): Sequential(\n    (0): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): AdaptiveAvgPool2d(output_size=1)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=512, out_features=15, bias=True)\n  )\n)\n\n\n\n\nCode\n# Install torchinfo, import if it's available\n# Torchinfo provides information complementary to what is provided by print(your_model) in PyTorch,\n# similar to Tensorflow's model.summary() API to view the visualization of the model, which is helpful while debugging your network.\n\ntry:\n  import torchinfo\nexcept:\n  !pip install torchinfo\n  import torchinfo\n\nfrom torchinfo import summary\nsummary(model, input_size=[1, 3, 64, 64])\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting torchinfo\n  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.8.0\n\n\n==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nResNet9                                  [1, 15]                   --\n├─Sequential: 1-1                        [1, 64, 64, 64]           --\n│    └─Conv2d: 2-1                       [1, 64, 64, 64]           1,792\n│    └─BatchNorm2d: 2-2                  [1, 64, 64, 64]           128\n│    └─ReLU: 2-3                         [1, 64, 64, 64]           --\n├─Sequential: 1-2                        [1, 128, 32, 32]          --\n│    └─Conv2d: 2-4                       [1, 128, 64, 64]          73,856\n│    └─BatchNorm2d: 2-5                  [1, 128, 64, 64]          256\n│    └─ReLU: 2-6                         [1, 128, 64, 64]          --\n│    └─MaxPool2d: 2-7                    [1, 128, 32, 32]          --\n├─Sequential: 1-3                        [1, 128, 32, 32]          --\n│    └─Sequential: 2-8                   [1, 128, 32, 32]          --\n│    │    └─Conv2d: 3-1                  [1, 128, 32, 32]          147,584\n│    │    └─BatchNorm2d: 3-2             [1, 128, 32, 32]          256\n│    │    └─ReLU: 3-3                    [1, 128, 32, 32]          --\n│    └─Sequential: 2-9                   [1, 128, 32, 32]          --\n│    │    └─Conv2d: 3-4                  [1, 128, 32, 32]          147,584\n│    │    └─BatchNorm2d: 3-5             [1, 128, 32, 32]          256\n│    │    └─ReLU: 3-6                    [1, 128, 32, 32]          --\n├─Sequential: 1-4                        [1, 256, 16, 16]          --\n│    └─Conv2d: 2-10                      [1, 256, 32, 32]          295,168\n│    └─BatchNorm2d: 2-11                 [1, 256, 32, 32]          512\n│    └─ReLU: 2-12                        [1, 256, 32, 32]          --\n│    └─MaxPool2d: 2-13                   [1, 256, 16, 16]          --\n├─Sequential: 1-5                        [1, 512, 8, 8]            --\n│    └─Conv2d: 2-14                      [1, 512, 16, 16]          1,180,160\n│    └─BatchNorm2d: 2-15                 [1, 512, 16, 16]          1,024\n│    └─ReLU: 2-16                        [1, 512, 16, 16]          --\n│    └─MaxPool2d: 2-17                   [1, 512, 8, 8]            --\n├─Sequential: 1-6                        [1, 512, 8, 8]            --\n│    └─Sequential: 2-18                  [1, 512, 8, 8]            --\n│    │    └─Conv2d: 3-7                  [1, 512, 8, 8]            2,359,808\n│    │    └─BatchNorm2d: 3-8             [1, 512, 8, 8]            1,024\n│    │    └─ReLU: 3-9                    [1, 512, 8, 8]            --\n│    └─Sequential: 2-19                  [1, 512, 8, 8]            --\n│    │    └─Conv2d: 3-10                 [1, 512, 8, 8]            2,359,808\n│    │    └─BatchNorm2d: 3-11            [1, 512, 8, 8]            1,024\n│    │    └─ReLU: 3-12                   [1, 512, 8, 8]            --\n├─Sequential: 1-7                        [1, 15]                   --\n│    └─AdaptiveAvgPool2d: 2-20           [1, 512, 1, 1]            --\n│    └─Flatten: 2-21                     [1, 512]                  --\n│    └─Dropout: 2-22                     [1, 512]                  --\n│    └─Linear: 2-23                      [1, 15]                   7,695\n==========================================================================================\nTotal params: 6,577,935\nTrainable params: 6,577,935\nNon-trainable params: 0\nTotal mult-adds (G): 1.52\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 24.12\nParams size (MB): 26.31\nEstimated Total Size (MB): 50.48\n==========================================================================================\n\n\n\n\nCode\ntorch.cuda.empty_cache() # Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n# official documentation --&gt; https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html#torch.cuda.empty_cache\n\nfor batch in train_dataloader:\n  images, labels = batch\n  print(f\"image shape: \", images.shape)\n  print(f\"images device: \", images.device)\n  preds = model(images.to(device))\n  print('preds.shape', preds.shape)\n  break\n\n\nimage shape:  torch.Size([32, 3, 64, 64])\nimages device:  cpu\npreds.shape torch.Size([32, 15])\n\n\n\n\nCode\nhistory = [evaluate(model.to(device),\n                    validation_dataloader)]\nhistory\n\n\n[{'validation_loss': 2.708531379699707,\n  'validation_accuracy': 0.06648936122655869}]\n\n\n\n\nCode\nhistory += fit(epochs=5,\n               lr=0.01,\n               model=model,\n               train_loader=train_dataloader,\n               val_loader=validation_dataloader,\n               opt_func=torch.optim.Adam)\n\n\n\n\nCode\ndef plot_accuracies(history):\n    accuracies = [x['validation_accuracy'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n\nplot_accuracies(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhistory += fit(epochs=10,\n               lr=0.01,\n               model=model,\n               train_loader=train_dataloader,\n               val_loader=validation_dataloader,\n               opt_func=torch.optim.Adam)\n\n\n\n\nCode\ndef plot_accuracies(history):\n    accuracies = [x['validation_accuracy'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')\n    plt.grid(True);  # Add grid lines\n\nplot_accuracies(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['validation_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n\nplot_losses(history)\n\n\n\n\n\n\n\n\n\n\n\nCode\nxb = img.unsqueeze(0).to(device)\n\n\n\n\nCode\n# Test with individual Images\ndef predict_image(img, model, classes):\n  # convert to a batch of 1\n  xb = img.unsqueeze(0).to(device) # It returns a new tensor with a dimension of size one inserted at the specified position dim. --&gt; https://www.geeksforgeeks.org/how-to-squeeze-and-unsqueeze-a-tensor-in-pytorch/\n  # Get the prediction from the model\n  yb = model(xb)\n  # pick index with highest probability\n  _, preds = torch.max(yb, dim=1)\n  # Retreive the class label\n  return classes[preds[0].item()]\n\n\n\n\nCode\ndef show_image_prediction(img, label):\n  plt.imshow(img.permute(1,2,0)) # Returns a view of the original tensor input with its dimensions permuted.--&gt;https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute\n  pred = predict_image(img, model, dataset.classes)\n  print(\"Target:\", dataset.classes[label])\n  print('Prediction:', pred)\n\n\n\n\nCode\nshow_image_prediction(*validation_dataset[23])\n\n\nTarget: Bean\nPrediction: Bean\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg, _ = validation_dataset[23]\nplt.imshow(img.permute(1,2,0))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndummy_input = torch.randn(1, 3, 64, 64).to(device)\ninput_names = [ \"actual_input\" ]\noutput_names = [ \"output\" ]\n\n\n\n\nCode\n!pip install onnx\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting onnx\n  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.6/14.6 MB 86.8 MB/s eta 0:00:00\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.22.4)\nRequirement already satisfied: protobuf&gt;=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\nRequirement already satisfied: typing-extensions&gt;=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.5.0)\nInstalling collected packages: onnx\nSuccessfully installed onnx-1.14.0\n\n\n\n\nCode\ntorch.onnx.export(model,\n                 dummy_input,\n                 \"ResNet9.onnx\",\n                 verbose=False,\n                 export_params=True,\n                 )\n\n\n============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n\n\n\n\nCode\n!pip install onnxruntime -q\n\n\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0.0/5.9 MB ? eta -:--:--     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺ 5.8/5.9 MB 174.3 MB/s eta 0:00:01     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.9/5.9 MB 93.9 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 5.6 MB/s eta 0:00:00\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 11.2 MB/s eta 0:00:00\n\n\n\n\nCode\nimport onnxruntime\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\n\n# Load the ONNX model\nmodel_path = 'ResNet9.onnx'\nsession = onnxruntime.InferenceSession(model_path)\n\n# Define the image transformation function\ndef image_transformation(img):\n    # Open the image\n    image = Image.open(img)\n\n    # Define the transformations\n    transform = transforms.Compose([\n        transforms.Resize((64, 64)),\n        transforms.ToTensor()\n    ])\n\n    # Apply the transformations to the image\n    transformed_image = transform(image)\n\n    return transformed_image\n\n# Define the class labels\nclass_labels = ['Cucumber', 'Capsicum', 'Papaya', 'Tomato', 'Cabbage', 'Pumpkin', 'Bitter_Gourd',\n                'Radish', 'Broccoli', 'Cauliflower', 'Bean', 'Carrot', 'Bottle_Gourd', 'Potato', 'Brinjal']\n\n# Define the function for inference and displaying the image\ndef infer_and_display_image(image_path):\n    # Perform inference\n    input_image = image_transformation(image_path)\n    input_tensor = np.expand_dims(input_image, axis=0)\n\n    # Run inference\n    input_name = session.get_inputs()[0].name\n    output_name = session.get_outputs()[0].name\n    output = session.run([output_name], {input_name: input_tensor})\n\n    # Postprocess the output\n    output = output[0]\n    predicted_class_index = np.argmax(output)\n    predicted_class = class_labels[predicted_class_index]\n\n    # Display the image\n    image = Image.open(image_path)\n    plt.imshow(image)\n\n    # Print the predicted class\n    print(\"Predicted class:\", predicted_class)\n\n\n\n\n\n\nCode\n# Call the function with the image path\nimage_path = '/content/vegetable-image-dataset/Vegetable Images/validation/Carrot/1202.jpg'\ninfer_and_display_image(image_path)\n\n\nPredicted class: Carrot\n\n\n\n\n\n\n\n\n\n\n\nCode\nimage_path = '/content/vegetable-image-dataset/Vegetable Images/validation/Pumpkin/1209.jpg'\ninfer_and_display_image(image_path)\n\n\nPredicted class: Pumpkin"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sadashiv Nandanikar",
    "section": "",
    "text": "Data Scientist @ Maruti Suzuki India Limited.\nExperienced Data Scientist | Machine Learning Specialist with a proven track record of driving business growth through data-driven insights, advanced analytics, and Python expertise."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "",
    "text": "Feature selection is a way of selecting the subset of the most relevant features from the original features set by removing the redundant, irrelevant, or noisy features.\nLet’s first understand some basics of feature selection."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#what-is-feature-selection",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#what-is-feature-selection",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "What is Feature Selection?",
    "text": "What is Feature Selection?\nA feature is an attribute that has an impact on a problem or is useful for the problem, and choosing the important features for the model is known as feature selection.\nEach machine learning process depends on feature engineering, which mainly contains two processes; which are Feature Selection and Feature Extraction. Although feature selection and extraction processes may have the same objective, both are completely different from each other.\n\nThe main difference between them is that feature selection is about selecting the subset of the original feature set, whereas feature extraction creates new features.\nFeature selection is a way of reducing the input variable for the model by using only relevant data in order to reduce overfitting in the model.\n\nSo, we can define feature Selection as, “It is a process of automatically or manually selecting the subset of most appropriate and relevant features to be used in model building.” Feature selection is performed by either including the important features or excluding the irrelevant features in the dataset without changing them."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#need-for-feature-selection",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#need-for-feature-selection",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "Need for Feature Selection",
    "text": "Need for Feature Selection\n\nAs we know, in machine learning, it is necessary to provide a pre-processed and good input dataset in order to get better outcomes. We collect a huge amount of data to train our model and help it to learn better.\nGenerally, the dataset consists of noisy data, irrelevant data, and some part of useful data.\nMoreover, the huge amount of data also slows down the training process of the model, and with noise and irrelevant data, the model may not predict and perform well.\nSo, it is very necessary to remove such noises and less-important data from the dataset and to do this, and Feature selection techniques are used.\n\nSelecting the best features helps the model to perform well. For example, Suppose we want to create a model that automatically decides which car should be crushed for a spare part, and to do this, we have a dataset. This dataset contains a Model of the car, Year, Owner’s name, Miles. So, in this dataset, the name of the owner does not contribute to the model performance as it does not decide if the car should be crushed or not, so we can remove this column and select the rest of the features(column) for the model building.\nBelow are some benefits of using feature selection in machine learning: - It helps in avoiding the [[Curse of Dimensionality]]. - It helps in the simplification of the model so that it can be easily interpreted by the researchers. - It reduces the training time. - It reduces overfitting hence enhance the generalization."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#feature-selection-techniques",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#feature-selection-techniques",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "Feature Selection Techniques",
    "text": "Feature Selection Techniques\nThere are mainly two types of Feature Selection techniques, which are: - Supervised Feature Selection technique Supervised Feature selection techniques consider the target variable and can be used for the labelled dataset. - Unsupervised Feature Selection technique Unsupervised Feature selection techniques ignore the target variable and can be used for the unlabeled dataset.\n\n\n\nFeature_Selection_Techniques_Drawing\n\n\nThere are mainly three techniques under supervised feature Selection:\n\n1. Wrapper Methods\nIn wrapper methodology, selection of features is done by considering it as a search problem, in which different combinations are made, evaluated, and compared with other combinations. It trains the algorithm by using the subset of features iteratively.\n\nOn the basis of the output of the model, features are added or subtracted, and with this feature set, the model has trained again.\nSome techniques of wrapper methods are:\n\nForward selection: Forward selection is an iterative process, which begins with an empty set of features. After each iteration, it keeps adding on a feature and evaluates the performance to check whether it is improving the performance or not. The process continues until the addition of a new variable/feature does not improve the performance of the model. ^71dca8\nBackward elimination: Backward elimination is also an iterative approach, but it is the opposite of forward selection. This technique begins the process by considering all the features and removes the least significant feature. This elimination process continues until removing the features does not improve the performance of the model.\nExhaustive Feature Selection: Exhaustive feature selection is one of the best feature selection methods, which evaluates each feature set as brute-force. It means this method tries & make each possible combination of features and return the best performing feature set.\nRecursive Feature Elimination: Recursive feature elimination is a recursive greedy optimization approach, where features are selected by recursively taking a smaller and smaller subset of features. Now, an estimator is trained with each set of features, and the importance of each feature is determined using coef_attribute or through a _feature_importances_attribute.\n\n\n\n2. Filter Methods\nIn Filter Method, features are selected on the basis of statistics measures. This method does not depend on the learning algorithm and chooses the features as a pre-processing step.\nThe filter method filters out the irrelevant feature and redundant columns from the model by using different metrics through ranking.\nThe advantage of using filter methods is that it needs low computational time and does not overfit the data.\n\nSome common techniques of Filter methods are as follows:\n\nInformation Gain: Information gain determines the reduction in entropy while transforming the dataset. It can be used as a feature selection technique by calculating the information gain of each variable with respect to the target variable.\nChi-square Test: Chi-square test is a technique to determine the relationship between the categorical variables. The chi-square value is calculated between each feature and the target variable, and the desired number of features with the best chi-square value is selected.\nFisher’s Score: Fisher’s score is one of the popular supervised technique of features selection. It returns the rank of the variable on the fisher’s criteria in descending order. Then we can select the variables with a large fisher’s score.\nMissing Value Ratio: The value of the missing value ratio can be used for evaluating the feature set against the threshold value. The formula for obtaining the missing value ratio is the number of missing values in each column divided by the total number of observations. The variable is having more than the threshold value can be dropped.\n\\[\\text{Missing Value Ratio} = \\frac{\\text{Number of Missing Values} \\times 100}{\\text{Total Number of Observations}}\n  \\]\n\n\n\n3. Embedded Methods\nEmbedded methods combined the advantages of both filter and wrapper methods by considering the interaction of features along with low computational cost. These are fast processing methods similar to the filter method but more accurate than the filter method.\n\nThese methods are also iterative, which evaluates each iteration, and optimally finds the most important features that contribute the most to training in a particular iteration. Some techniques of embedded methods are:\n\nRegularization: Regularization adds a penalty term to different parameters of the machine learning model for avoiding overfitting in the model. This penalty term is added to the coefficients; hence it shrinks some coefficients to zero. Those features with zero coefficients can be removed from the dataset. The types of regularization techniques are L1 Regularization (Lasso Regularization) or Elastic Nets (L1 and L2 regularization).\nRandom Forest Importance: Different tree-based methods of feature selection help us with feature importance to provide a way of selecting features. Here, feature importance specifies which feature has more importance in model building or has a great impact on the target variable. Random Forest is such a tree-based method, which is a type of bagging algorithm that aggregates a different number of decision trees. It automatically ranks the nodes by their performance or decrease in the impurity (Gini impurity) over all the trees. Nodes are arranged as per the impurity values, and thus it allows to pruning of trees below a specific node. The remaining nodes create a subset of the most important features."
  },
  {
    "objectID": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#how-to-choose-a-feature-selection-method",
    "href": "blogs/feature_selection_technique/Feature Selection Techniques in Machine Learning.html#how-to-choose-a-feature-selection-method",
    "title": "Feature Selection Techniques in Machine Learning",
    "section": "How to choose a Feature Selection Method?",
    "text": "How to choose a Feature Selection Method?\nFor machine learning engineers, it is very important to understand that which feature selection method will work properly for their model. The more we know the datatypes of variables, the easier it is to choose the appropriate statistical measure for feature selection.\n\n\n\nFeature_Selection_Flow_Chart\n\n\nTo know this, we need to first identify the type of input and output variables. In machine learning, variables are of mainly two types:\n\nNumerical Variables: Variable with continuous values such as integer, float\nCategorical Variables: Variables with categorical values such as Boolean, ordinal, nominals.\n\nBelow are some univariate statistical measures, which can be used for filter-based feature selection:\n\nNumerical Input, Numerical Output: Numerical Input variables are used for predictive regression modelling. The common method to be used for such a case is the Correlation coefficient.\n\nPearson’s correlation coefficient (For linear Correlation).\nSpearman’s rank coefficient (for non-linear correlation).\n\n Numerical Input, Categorical Output: Numerical Input with categorical output is the case for classification predictive modelling problems. In this case, also, correlation-based techniques should be used, but with categorical output.\n\nANOVA correlation coefficient (linear).\nKendall’s rank coefficient (nonlinear).\n\nCategorical Input, Numerical Output: This is the case of regression predictive modelling with categorical input. It is a different example of a regression problem. We can use the same measures as discussed in the above case but in reverse order.\nCategorical Input, Categorical Output: This is a case of classification predictive modelling with categorical Input variables.\nThe commonly used technique for such a case is Chi-Squared Test. We can also use Information gain in this case.\n\n\n\n\n\n\n\n\n\nInput Variable\nOutput Variable\nFeature Selection technique\n\n\n\n\nNumerical\nNumerical\n- Pearson’s correlation coefficient (For linear Correlation).- Pearson’s correlation coefficient (For linear Correlation)\n\n\nNumerical\nCategorical\n- ANOVA correlation coefficient (linear).  - Kendall’s rank coefficient (nonlinear).\n\n\nCategorical\nNumerical\n- Kendall’s rank coefficient (linear).  - ANOVA correlation coefficient (nonlinear).\n\n\nCategorical\nCategorical\n- Chi-Squared test (contingency tables).  - Mutual Information."
  },
  {
    "objectID": "blogs/Supervised_Learning_Algorithms/Supervised Learning Algorithms.html",
    "href": "blogs/Supervised_Learning_Algorithms/Supervised Learning Algorithms.html",
    "title": "Supervised Learning Algorithms",
    "section": "",
    "text": "What’s Supervised Learning? Imagine teaching a computer like a teacher instructs a student. In supervised learning, we provide the computer with labeled examples. It’s like showing it the right answers and letting it figure out how to get there.\n\n\n\nsupervised learning\n\n\nWhy Do We Use It?\nSupervised learning helps the computer make predictions, classify things, and make decisions based on past experiences.\nMeet the Superstars!\n\n[[Linear Regression in Machine Learning|Linear Regression]]: Think of it as drawing a straight line through dots on a graph to make predictions, like guessing a house’s price based on its size.\n[[Decision Tree|Decision Trees]]: Imagine playing 20 Questions. It’s like that but for sorting things, such as deciding if an email is spam or not.\nSupport Vector Machines (SVM): Picture separating different groups of data with a clear line. It’s used in things like sorting images or categorizing text.\n\nHow It Works\n\nTraining: We teach the algorithm using a dataset with examples and their correct answers. It learns by finding patterns in this training data.\nTesting: After training, we test it on new, unseen data to see how well it can predict or classify.\n\nIn the Real World\n\nPredicting stock prices based on past trends.\nDeciding if an email is junk or not.\nRecognizing handwritten numbers in ZIP codes.\n\nPros and Cons\n\nPros: It’s widely used and can be very accurate if the training data is good.\nCons: You need labeled data (which can be costly), and it might not work well if the data is messy or the problem is tricky.\n\nTakeaway\nSupervised learning is like training a pet with treats. It’s essential in machine learning, especially when you know what the right answers should be. It’s like the foundation of our machine learning journey!\n\n\nReference Reading\n# Supervised Learning: Algorithms, Examples, and How It Works"
  }
]